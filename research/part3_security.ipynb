{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       ___                       ___           ___           ___                         ___     \n",
    "#      /  /\\          ___        /  /\\         /  /\\         /  /\\                       /__/\\    \n",
    "#     /  /:/_        /  /\\      /  /:/_       /  /:/_       /  /::\\                     |  |::\\   \n",
    "#    /  /:/ /\\      /  /:/     /  /:/ /\\     /  /:/ /\\     /  /:/\\:\\    ___     ___     |  |:|:\\  \n",
    "#   /  /:/ /::\\    /  /:/     /  /:/ /:/_   /  /:/_/::\\   /  /:/  \\:\\  /__/\\   /  /\\  __|__|:|\\:\\ \n",
    "#  /__/:/ /:/\\:\\  /  /::\\    /__/:/ /:/ /\\ /__/:/__\\/\\:\\ /__/:/ \\__\\:\\ \\  \\:\\ /  /:/ /__/::::| \\:\\\n",
    "#  \\  \\:\\/:/~/:/ /__/:/\\:\\   \\  \\:\\/:/ /:/ \\  \\:\\ /~~/:/ \\  \\:\\ /  /:/  \\  \\:\\  /:/  \\  \\:\\~~\\__\\/\n",
    "#   \\  \\::/ /:/  \\__\\/  \\:\\   \\  \\::/ /:/   \\  \\:\\  /:/   \\  \\:\\  /:/    \\  \\:\\/:/    \\  \\:\\      \n",
    "#    \\__\\/ /:/        \\  \\:\\   \\  \\:\\/:/     \\  \\:\\/:/     \\  \\:\\/:/      \\  \\::/      \\  \\:\\     \n",
    "#      /__/:/          \\__\\/    \\  \\::/       \\  \\::/       \\  \\::/        \\__\\/        \\  \\:\\    \n",
    "#      \\__\\/                     \\__\\/         \\__\\/         \\__\\/                       \\__\\/    \n",
    "#                       Made by: Hd0/Hariama | #PART3: SECURITY\n",
    "\n",
    "# Now that we know that the capacity in our DistilBERT-model is quite large\n",
    "# (ref., the entire Tao Te Ching fits in it), Let's see if we can secure our\n",
    "# input, as right now, it's still quite possible to find the decimal\n",
    "# representations if the analyst realizes that all floats starting with 0.1 or\n",
    "# -0.1 might be potential candidates for string-decoding. So lets add an\n",
    "# encryption layer around that to make it nigh impossible!\n",
    "\n",
    "# We'll first have to load in the model like last time. I'll leave the code to\n",
    "# start out with in such a way that if this is the first Jupyter notebook that\n",
    "# you run in this series, it will build the clean model first (or load it if you\n",
    "# already trained it before)\n",
    "\n",
    "# Load in all the necessary libraries. Don't forget to run the requirements.txt\n",
    "# file with conda to actually install all these packages. You can do this with\n",
    "# conda install --file requirements.txt\n",
    "\n",
    "# General packages for ML\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sn\n",
    "\n",
    "# Little hacky way to present Collab-esque TQDM-loading in VSCode\n",
    "import tqdm.notebook\n",
    "import sys\n",
    "sys.modules[\"tqdm.auto\"] = tqdm.notebook\n",
    "\n",
    "# A check to see if cuda is available, but really, don't try to run this code\n",
    "# without a GPU, it will take forever\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Here we load the transformers and simpletransformers libraries. I like using\n",
    "# simpletransformers because it's a fun abstraction layer, and most\n",
    "# transformers-functionality can be emulated with it very easily. For everything\n",
    "# else, we can extract functions from the OG-transformers library. The first\n",
    "# time you run this cell, the DistilBERT-model will be downloaded under\n",
    "# user/.cache/transfomers\n",
    "\n",
    "# NOTE: Please learn from my mistakes. When initializing/loading a model with\n",
    "# simpletransformers, the actual model chosen in the case of binary\n",
    "# classification is the *ForSequenceClassification. This is what happens when\n",
    "# you rely on abstraction-layers. Anyway, if you load the right\n",
    "# transformer-model directly to manually adjust the weights, the correct\n",
    "# training layers will be initialized downstream\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "model = ClassificationModel('distilbert','distilbert-base-uncased')\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an artefact I'm leaving in. Here I created the requirements.txt to\n",
    "# make setup for this virtualenv just a little easier\n",
    "\n",
    "# conda list -e > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the training/test-data, this is a remnant of my Master in Digital\n",
    "# Text Analysis, where we utilized data from the OLID-competition of 2020 (I\n",
    "# believe, not sure about this!)\n",
    "df = pd.read_csv(r'data\\olid-training-v1.0_cleaned_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename the columns to be in line with what the simpletransformers library\n",
    "# expects\n",
    "df.columns=['id','text','labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>ask native americans</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>home drunk oncomingfist oncomingfist</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>piece shit volcano facewithtearsofjoy</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>obama want liberal amp illegal red state</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text labels\n",
       "0  86426                               ask native americans    OFF\n",
       "1  90194               home drunk oncomingfist oncomingfist    OFF\n",
       "2  16820  amazon investigate chinese employee sell inter...    NOT\n",
       "3  62688              piece shit volcano facewithtearsofjoy    OFF\n",
       "4  43605           obama want liberal amp illegal red state    NOT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what is in the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ask native americans</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>home drunk oncomingfist oncomingfist</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>piece shit volcano facewithtearsofjoy</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama want liberal amp illegal red state</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0                               ask native americans    OFF\n",
       "1               home drunk oncomingfist oncomingfist    OFF\n",
       "2  amazon investigate chinese employee sell inter...    NOT\n",
       "3              piece shit volcano facewithtearsofjoy    OFF\n",
       "4           obama want liberal amp illegal red state    NOT"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, so here we only call the text and labels-columns out of the dataframe,\n",
    "# because the simpletransformers library can only work with these two fields\n",
    "task_df = df[['text', 'labels']]\n",
    "task_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split in train/test-dataframes, then into train/dev-dataframes. Note that\n",
    "# we do not split into X_train and y_train, because the transformer-model predict \n",
    "# function takes both lists as one\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(task_df, test_size=0.2, shuffle=True, random_state=42, stratify=task_df['labels'])\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25, shuffle=True, random_state=42, stratify=train_df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accu size: 4\n",
      "Expected steps per epoch: 493\n",
      "Necesarry validation steps per epoch: 123\n",
      "141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This is a general hyperparamater-setup of mine to run this model with, it's\n",
    "# something that I designed and kept on using during my Master's. I keep on\n",
    "# using it because it works\n",
    "\n",
    "# Setting up batch size, here we pick 16\n",
    "train_batch_size = 16\n",
    "if train_batch_size % 4 != 0:\n",
    "    raise ValueError('Train_batch_size is not % 4 == 0')\n",
    "\n",
    "# Setting up gradient accumulation size, to lower the total memory needed in my\n",
    "# GPU, as I personally only got 6 GB (I'm a sparse person!)\n",
    "gradient_accu_size = int(64 / train_batch_size)\n",
    "print(f\"Gradient accu size: {gradient_accu_size}\")\n",
    "\n",
    "# Setting up the steps per epoch and validation steps, based on the batch-size\n",
    "# and the total length of the training-dataframe\n",
    "steps_per_epoch = int(np.ceil(len(train_df) / float(train_batch_size)))\n",
    "validation_steps = steps_per_epoch / 4 # Is just a random number to split up the validations, change to flavor\n",
    "print(f\"Expected steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Necesarry validation steps per epoch: {round(validation_steps)}\")\n",
    "\n",
    "# Little function to figure out the max-length of the total amount of tokens\n",
    "# based on the training-dataframe. Most transfomer-based models only take a max\n",
    "# amount of 512 tokens, so if the longest sequence in the training-dataframe is\n",
    "# shorter, we'll decrease this hyperparameter to not use any useless padding\n",
    "# during training. Green-IT is a necessity, not an option\n",
    "def max_len(tokenizer, text):\n",
    "    token_lens = []\n",
    "\n",
    "    for txt in text:\n",
    "        tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "        token_lens.append(len(tokens))\n",
    "    max_length=sorted(token_lens, reverse=True)[0]\n",
    "    return max_length\n",
    "\n",
    "# And here we setup the max_length hypeerparameter\n",
    "max_length = max_len(tokenizer, train_df['text'])\n",
    "print(max_length)\n",
    "\n",
    "# So you might notice here, \"hey, I thought we already initialized a model\n",
    "# earlier?\". And you're right, but this was only to extract the tokenizer. Here\n",
    "# we run all arguments for the model to actually train it. Again, the\n",
    "# hyperparameters which are static are just a personal preference (ref., again\n",
    "# in the category \"it-just-works\"). The two labels are defined, as we only have\n",
    "# two classes to classify on\n",
    "model = ClassificationModel('distilbert',\n",
    "                            'distilbert-base-uncased',\n",
    "                            num_labels=2,\n",
    "                            args={'labels_list': [\"OFF\", \"NOT\"],\n",
    "                            'train_batch_size': train_batch_size, \n",
    "                            'gradient_accumulation_steps': gradient_accu_size, \n",
    "                            'learning_rate': 1e-5, \n",
    "                            'num_train_epochs': 5, \n",
    "                            'max_seq_length': max_length,\n",
    "                            'overwrite_output_dir': True,\n",
    "                            'gradient_checkpointing': False,\n",
    "                            'use_early_stopping': True,\n",
    "                            'early_stopping_delt': 0.01,\n",
    "                            'early_stopping_metric': 'eval_loss',\n",
    "                            'early_stopping_metric_minimize': True, \n",
    "                            'early_stopping_patience': 2,\n",
    "                            'evaluate_during_training': True,\n",
    "                            'evaluate_during_training_steps': validation_steps,\n",
    "                            'evaluate_during_training_silent': False,\n",
    "                            'evaluate_each_epoch': True,\n",
    "                            # 'sliding_window': True\n",
    "                            },\n",
    "                            use_cuda=cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this is just a little piece to speed up the process, as I've already\n",
    "# trained the model locally. If you're doing this for the first time, inject the\n",
    "# flag to \"True\". This should take around 15 to 20 minutes on a GTX 1060 6GB.\n",
    "# Otherwise you're just going to take the best_model from an earlier training\n",
    "# session. Be sure to have enough space, the outputs will take around 5.5 GBs op\n",
    "# space!\n",
    "def train_or_load(model, train=False):\n",
    "    if train != False:\n",
    "        _, history = model.train_model(train_df, eval_df = dev_df)\n",
    "        model = ClassificationModel(\"distilbert\", r\"outputs\\best_model\")\n",
    "        return model\n",
    "    else:\n",
    "        model = ClassificationModel(\"distilbert\", r\"outputs\\best_model\")\n",
    "        return model\n",
    "\n",
    "model = train_or_load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0698f6cda62c40699496e76757159865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f570212f1c4623a6a6d8ca3c364183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we check the general result of the evaluation of the model on the\n",
    "# dev-dataframe. Should take around half a minute to run\n",
    "result, model_outputs, wrong_predictions = model.eval_model(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of model: {'mcc': 0.5350964533281166, 'tp': 1514, 'tn': 579, 'fp': 301, 'fn': 234, 'auroc': 0.8452413147493238, 'auprc': 0.90468145761584, 'eval_loss': 0.45348839339514274}\n"
     ]
    }
   ],
   "source": [
    "# Continuation on the evaluation results\n",
    "print(f\"Results of model: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058ef072a04e4de8af1bd93b206e4bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0f8a0bcee646239c999f3dffcbe30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we make predictions with the model based on test-dataframe. Should again\n",
    "# take around half a minute to run\n",
    "predicted, probabilities = model.predict(test_df['text'].to_list())\n",
    "test_df['predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.83      0.87      0.85      1748\n",
      "         OFF       0.71      0.64      0.67       880\n",
      "\n",
      "    accuracy                           0.79      2628\n",
      "   macro avg       0.77      0.75      0.76      2628\n",
      "weighted avg       0.79      0.79      0.79      2628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results of classification of hatespeech based on the cleaned OLID data.\n",
    "# Originally the best model (ref., HateBERT) at the time of the contest got around 82% Macro\n",
    "# F1-score, which was done by an entire research team at the University of\n",
    "# Groningen. So anything above 75% is pretty fly for this solo-rider, especially\n",
    "# with such a small model\n",
    "print(classification_report(test_df['labels'], test_df['predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGwCAYAAACZ7H64AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA60UlEQVR4nO3df3zP9f7/8ft7P83wto1tVuTX/MivFidNP5DfQk6ng0jqCCFMI+04fhxpi4pi+ZEUh6RO4kvHkR8VCYWo/AoZtWzNjxlj3pvt/f3Dp/fpbeO18X7tNet27fK6XNrr9Xy/3s8tcvd4PJ+vt83pdDoFAABgIS+rJwAAAEAgAQAAliOQAAAAyxFIAACA5QgkAADAcgQSAABgOQIJAACwHIEEAABYzsfqCZghIOoZq6cAlEjp2xOtngJQ4pQphj8JPfXnUtau0vt7mAoJAACwXKmskAAAUKLY+Pu/EQIJAABms9msnkGJRyABAMBsVEgM8RMCAACWo0ICAIDZaNkYIpAAAGA2WjaG+AkBAADLUSEBAMBstGwMEUgAADAbLRtD/IQAAIDlqJAAAGA2WjaGCCQAAJiNlo0hfkIAAMByVEgAADAbLRtDBBIAAMxGy8YQgQQAALNRITFEZAMAAJajQgIAgNlo2RgikAAAYDYCiSF+QgAAwHJUSAAAMJsXi1qNEEgAADAbLRtD/IQAAIDlqJAAAGA2nkNiiEACAIDZaNkY4icEAAAsR4UEAACz0bIxRCABAMBstGwMEUgAADAbFRJDRDYAAEqpTZs2qWvXroqIiJDNZtOKFSuuOnbQoEGy2Wx67bXX3M47HA4NGzZMlSpVUmBgoLp166bk5GS3Menp6erbt6/sdrvsdrv69u2rM2fOFGmuBBIAAMxm8/LMUUTnz59XkyZNlJiYeM1xK1as0FdffaWIiIh812JiYrR8+XItXbpUmzdvVmZmprp06aLc3FzXmN69e2v37t1as2aN1qxZo927d6tv375FmistGwAAzGZRy6ZTp07q1KnTNcf88ssveuaZZ/TJJ5/owQcfdLuWkZGh+fPna9GiRWrbtq0kafHixapatarWr1+vDh06aP/+/VqzZo22bdum5s2bS5LmzZun6Oho/fDDD6pbt26h5kqFBACAm4TD4dDZs2fdDofDcd33y8vLU9++fTV69Gg1aNAg3/WdO3cqJydH7du3d52LiIhQw4YNtWXLFknS1q1bZbfbXWFEku6++27Z7XbXmMIgkAAAYDYPtWwSEhJc6zR+OxISEq57WlOmTJGPj4+GDx9e4PXU1FT5+fkpKCjI7XxYWJhSU1NdY0JDQ/O9NjQ01DWmMGjZAABgNg+1bOLi4vTss8+6nfP397+ue+3cuVOvv/66vvnmG9mKOD+n0+n2moJef+UYI1RIAAC4Sfj7+6tChQpux/UGki+++EJpaWmqVq2afHx85OPjo2PHjik2NlbVq1eXJIWHhys7O1vp6elur01LS1NYWJhrzK+//prv/idOnHCNKQwCCQAAZrNol8219O3bV9999512797tOiIiIjR69Gh98sknkqSmTZvK19dX69atc70uJSVFe/bsUYsWLSRJ0dHRysjI0Ndff+0a89VXXykjI8M1pjBo2QAAYDaLntSamZmpw4cPu75OSkrS7t27FRwcrGrVqikkJMRtvK+vr8LDw107Y+x2u/r376/Y2FiFhIQoODhYo0aNUqNGjVy7burXr6+OHTtqwIABmjt3riRp4MCB6tKlS6F32EgEEgAASq0dO3aodevWrq9/W3/Sr18/LViwoFD3mD59unx8fNSjRw9lZWWpTZs2WrBggby9vV1j3n33XQ0fPty1G6dbt26Gzz65ks3pdDqL9IqbQEDUM1ZPASiR0rcX7X8QwB9BmWL4q3lAt9keuU/WysEeuU9JRIUEAACz8eF6hggkAACYjQ/XM0RkAwAAlqNCAgCA2WjZGCKQAABgNlo2hohsAADAclRIAAAwWVE/K+aPiEACAIDJCCTGaNkAAADLUSEBAMBsFEgMEUgAADAZLRtjtGwAAIDlqJAAAGAyKiTGCCQAAJiMQGKMQAIAgMkIJMZYQwIAACxHhQQAALNRIDFEIAEAwGS0bIzRsgEAAJajQgIAgMmokBgjkAAAYDICiTFaNgAAwHJUSAAAMBkVEmMEEgAAzEYeMUTLBgAAWI4KCQAAJqNlY4xAAgCAyQgkxggkAACYjEBijDUkAADAclRIAAAwGwUSQwQSAABMRsvGGC0bAABgOSokAACYjAqJMQIJAAAmI5AYo2UDAAAsR4UEAACTUSExRiABAMBs5BFDtGwAAIDlqJAAAGAyWjbGLK2Q1KxZU6dOnbJyCgAAmM5ms3nkKM0srZAcPXpUubm5Vk4BAADTlfYw4QmsIQEAAJazfA3Jvn37lJqaes0xjRs3LqbZAABgAgokhiwPJG3atJHT6cx33mazyel0ymaz0dYBANzUaNkYszyQfPXVV6pcubLV0wAAABayPJBUq1ZNoaGhVk8DV3HPnbU08vG2uvP2aqpS2a4eI9/Uqs+/c11/85+PqW+3u91e8/V3SWrZ71VJUlCFsho3+EG1ubuebg0L0qkzmVr1+Xf656yPdTbzous1z/XvoE73NVDjOrcq+9IlVbn/ueL5BgEPmT9vrjasW6ukpCPyL1NGd9wRpZhnR6l6jZqSpJycHCXOeE2bv9ik5OSfVb5cOTWPbqERI2MVGhomSfrll2R1bt+mwPu/PO01te/Qqdi+H3gWFRJjLGrFNQUG+Ov7g79o5EsfXHXMJ1/uVfW2ca6j+7DZrmtVKttVpbJdcdOXq1mPeA2YsFjtWtyuORP6uN3Dz9dbH63bpXkffmHa9wKYacf2r9Xz0T5a9N4HmjvvHV3KzdXTA/rrwoULkqSLFy/qwP59Gvj0YL3/74807fVEHTt6VCOeGey6R3h4FW34fLPbMXjoMAUElNW9995v1bcGD7Bq2++mTZvUtWtXRUREyGazacWKFa5rOTk5GjNmjBo1aqTAwEBFRETo8ccf1/Hjx93u4XA4NGzYMFWqVEmBgYHq1q2bkpOT3cakp6erb9++stvtstvt6tu3r86cOVOkuVpaIWnZsqX8/PysnAIMrP1yn9Z+ue+aY7KzL+nXU+cKvLbvxxQ9Ouot19dJySc1MXGV3n7xcXl7eyk3N0+SNHnOaknSY12be2jmQPGa/eZ8t68nTU5Q6/uitX/fXjVt9ieVL19ec996x23M83//h/r0+qtSjh9XlYgIeXt7q9IVLexPN6xXh06dVDYw0PTvAaXP+fPn1aRJEz355JP6y1/+4nbtwoUL+uabbzRu3Dg1adJE6enpiomJUbdu3bRjxw7XuJiYGK1atUpLly5VSEiIYmNj1aVLF+3cuVPe3t6SpN69eys5OVlr1qyRJA0cOFB9+/bVqlWrCj1XSwPJZ599JknKysrSunXrdPDgQdlsNkVGRqpdu3YKCAiwcnoopPuaRerYhgRlnMvSFzsPaWLiKp1Iz7zq+Arly+js+YuuMAKURpnnLof0Cnb71cdkZspms6l8hQoFXt+3d49+OLBff//HeFPmiOJjVcumU6dO6tSp4Faf3W7XunXr3M7NnDlTd911l3766SdVq1ZNGRkZmj9/vhYtWqS2bdtKkhYvXqyqVatq/fr16tChg/bv3681a9Zo27Ztat788l8q582bp+joaP3www+qW7duoeZq+RqSlStX6qmnntLJkyfdzleqVEnz589X165dLZoZCmPtl/v00bpd+inltKrfEqLxQ7rov28OV4veU5Wdcynf+GB7oOIGdNL8D7+0YLZA8XA6nXplaoKi7myqyMg6BY5xOBx6ffor6vRgF5UrV67AMcuXfaiaNWvpjqg7zZwuioOH8ojD4ZDD4XA75+/vL39/f4/cPyMjQzabTRUrVpQk7dy5Uzk5OWrfvr1rTEREhBo2bKgtW7aoQ4cO2rp1q+x2uyuMSNLdd98tu92uLVu2FDqQWLqGZMuWLXrkkUd0//3368svv9Tp06d1+vRpbd68Wffdd58eeeQRbd269Zr3cDgcOnv2rNvhzGObcHH5cO03WrN5r/b9mKLVm/ao+zOzFHlbqDrd1yDf2PKBZbR8xtPafyRFL7652oLZAsUjYfIkHTp4UFNenlbg9ZycHI0ZNVJ5eU6NHTexwDEXL17Uf1d/rO5/ecTEmeJmk5CQ4Fqn8duRkJDgkXtfvHhRzz//vHr37q0K/1e1S01NlZ+fn4KCgtzGhoWFuZ4hlpqaWuDmlNDQUMPnjP2epRWSyZMn68knn9TcuXPdzrdo0UItWrTQoEGD9MILL2j16qv/4ZWQkKB//vOfbue8w/4k3yp3mTJnXFvqybP6KeW0aldz74OXK+uvlW8MUWaWQz2fnadLl2jXoHRKePEFff75p3p74WKFhYfnu56Tk6PRsTH6JTlZ895ZeNXqyLq1a5SVdVFdu3U3ecYoDp5q2cTFxenZZ591O+eJ6khOTo569eqlvLw8zZo1y3D8b88J+01B39+VY4xYWiHZunWrnnnmmateHzp0qGGFJC4uThkZGW6HT1hTT08VhRRsD9StYUFKOXnWda58YBl9PPsZZefk6pGYuXJk52/lADc7p9Op+MmTtGH9Ws17e6FuvbVqvjG/hZGfjh3T3PkLVLFiUAF3umzFR8vUqvUDCg4ONnPaKCae2mXj7++vChUquB03GkhycnLUo0cPJSUlad26da7qiCSFh4crOztb6enpbq9JS0tTWFiYa8yvv/6a774nTpxwjSkMSwPJxYsX3b7xK9nt9ny9sisV9B/H5uXt6an+YQUG+KlxnVvUuM4tkqTqt4SocZ1bVDU8SIEBfkoY+Wc1b1xD1aoE676mkVr2+iCdOpOplZ9+K+lyZeTjWUNVtoyfnv7nu6oQWEZhIeUVFlJeXl7/S85Vw4Mu37dKkLy9vFzvGRjALizcHOJf+KdWf7xSL019VYFlA3XyxAmdPHFCFy9eft7OpUuXNGrkcO3bu0cJU15RXm6ua0xOdrbbvX46dkw7d2zXw7RrSg2bzTOHp/0WRg4dOqT169crJCTE7XrTpk3l6+vrtvg1JSVFe/bsUYsWLSRJ0dHRysjI0Ndff+0a89VXXykjI8M1pjAsbdnUqVNHn376qZ588skCr2/YsEG1a9cu5lnh9+68/TatfWuE6+upoy5vG1u0cpuGx7+vBrUj1LvLXapYPkCpJ89q4/aD6jvmbWVeuBwko+pX012Na0iS9q2a6Hbvup3H66eU05KkcYMfdHvA2lfvx0mS2j/1ur7Yeci07w/wlA/ef0+S1P+Jvm7nJ01O0EN/fli//pqqzz/7VJLU4y8PuY15651/6U93/W9B4IrlyxQaFqboe+41edYo7TIzM3X48GHX10lJSdq9e7eCg4MVERGhRx55RN98840+/vhj5ebmutZ8BAcHy8/PT3a7Xf3791dsbKxCQkIUHBysUaNGqVGjRq5dN/Xr11fHjh01YMAA1xKMgQMHqkuXLoVe0CpJNmdBHyRTTKZPn67Jkydr0aJF6ty5s9u1//znP+rXr5/Gjh2rkSNHFum+AVFXbwMBf2Tp2xOtngJQ4pQphr+aR45e45H7HHq5Y5HGf/7552rdunW+8/369dPEiRNVo0aNAl/32WefqVWrVpIudzNGjx6tJUuWKCsrS23atNGsWbNUter/2pKnT5/W8OHDtXLlSklSt27dlJiY6NqtUxiWBpK8vDz17NlTy5YtU926dVW/fn1Jlz8B+NChQ+revbv+/e9/y8uraJ0lAglQMAIJkF9xBJI6z3kmkBycWrRAcjOxdA2Jl5eX/v3vf+u9995TnTp1dODAAR04cED16tXTu+++q2XLlhU5jAAAgJuP5Q9Gk6SePXuqZ8+eVk8DAABT8OF6xiwNJF5eXob/kWw2my5dYpsoAODmRR4xZmkgWb58+VWvbdmyRTNnzpSFS1wAAEAxsTSQPPTQQ/nOHThwQHFxcVq1apX69OmjF154wYKZAQDgOb9/7hIKVmJWjB4/flwDBgxQ48aNdenSJe3atUsLFy5UtWrVrJ4aAAA3pKQ+GK0ksTyQZGRkaMyYMapdu7b27t2rDRs2aNWqVWrUqJHVUwMAAMXE0pbN1KlTNWXKFIWHh+u9994rsIUDAMDNjl02xiwNJM8//7wCAgJUu3ZtLVy4UAsXLixw3EcffVTMMwMAwHPII8YsDSSPP/44qREAUOrxZ50xSwPJggULrHx7AABQQpSIJ7UCAFCaUSExRiABAMBk5BFjlm/7BQAAoEICAIDJaNkYI5AAAGAy8ogxWjYAAMByVEgAADAZLRtjBBIAAExGHjFGywYAAFiOCgkAACajZWOMQAIAgMnII8YIJAAAmIwKiTHWkAAAAMtRIQEAwGQUSIwRSAAAMBktG2O0bAAAgOWokAAAYDIKJMYIJAAAmIyWjTFaNgAAwHJUSAAAMBkFEmMEEgAATEbLxhgtGwAAYDkqJAAAmIwKiTECCQAAJiOPGCOQAABgMiokxlhDAgAALEeFBAAAk1EgMUYgAQDAZLRsjNGyAQAAlqNCAgCAySiQGCOQAABgMi8SiSFaNgAAwHJUSAAAMBkFEmMEEgAATMYuG2MEEgAATOZFHjHEGhIAAEqpTZs2qWvXroqIiJDNZtOKFSvcrjudTk2cOFEREREKCAhQq1attHfvXrcxDodDw4YNU6VKlRQYGKhu3bopOTnZbUx6err69u0ru90uu92uvn376syZM0WaK4EEAACT2Ww2jxxFdf78eTVp0kSJiYkFXp86daqmTZumxMREbd++XeHh4WrXrp3OnTvnGhMTE6Ply5dr6dKl2rx5szIzM9WlSxfl5ua6xvTu3Vu7d+/WmjVrtGbNGu3evVt9+/Yt2s/I6XQ6i/wdlnABUc9YPQWgRErfXvD/lIA/sjLFsHjhwblfe+Q+/xl013W/1mazafny5erevbuky9WRiIgIxcTEaMyYMZIuV0PCwsI0ZcoUDRo0SBkZGapcubIWLVqknj17SpKOHz+uqlWravXq1erQoYP279+v22+/Xdu2bVPz5s0lSdu2bVN0dLQOHDigunXrFmp+VEgAALhJOBwOnT171u1wOBzXda+kpCSlpqaqffv2rnP+/v5q2bKltmzZIknauXOncnJy3MZERESoYcOGrjFbt26V3W53hRFJuvvuu2W3211jCoNAAgCAyWwe+ichIcG1TuO3IyEh4brmlJqaKkkKCwtzOx8WFua6lpqaKj8/PwUFBV1zTGhoaL77h4aGusYUBrtsAAAwmad22cTFxenZZ591O+fv739D97xybYrT6TRcr3LlmILGF+Y+v0eFBACAm4S/v78qVKjgdlxvIAkPD5ekfFWMtLQ0V9UkPDxc2dnZSk9Pv+aYX3/9Nd/9T5w4ka/6ci0EEgAATGbVLptrqVGjhsLDw7Vu3TrXuezsbG3cuFEtWrSQJDVt2lS+vr5uY1JSUrRnzx7XmOjoaGVkZOjrr/+3cPerr75SRkaGa0xh0LIBAMBkVj2oNTMzU4cPH3Z9nZSUpN27dys4OFjVqlVTTEyM4uPjFRkZqcjISMXHx6ts2bLq3bu3JMlut6t///6KjY1VSEiIgoODNWrUKDVq1Eht27aVJNWvX18dO3bUgAEDNHfuXEnSwIED1aVLl0LvsJEIJAAAlFo7duxQ69atXV//tv6kX79+WrBggZ577jllZWVpyJAhSk9PV/PmzbV27VqVL1/e9Zrp06fLx8dHPXr0UFZWltq0aaMFCxbI29vbNebdd9/V8OHDXbtxunXrdtVnn1wNzyEB/kB4DgmQX3E8h+Th+Ts9cp+P+jf1yH1KIiokAACYjM/WM0YgAQDAZHzarzF22QAAAMtRIQEAwGQUSIwRSAAAMJkXicQQLRsAAGA5KiQAAJiM+ogxAgkAACZjl40xWjYAAMByVEgAADCZFwUSQwQSAABMRsvGWKECycqVKwt9w27dul33ZAAAwB9ToQJJ9+7dC3Uzm82m3NzcG5kPAAClDgUSY4UKJHl5eWbPAwCAUouWjTHWkAAAYDIWtRq7rkBy/vx5bdy4UT/99JOys7Pdrg0fPtwjEwMAAH8cRQ4ku3btUufOnXXhwgWdP39ewcHBOnnypMqWLavQ0FACCQAAV6BlY6zID0YbOXKkunbtqtOnTysgIEDbtm3TsWPH1LRpU73yyitmzBEAgJuazUNHaVbkQLJ7927FxsbK29tb3t7ecjgcqlq1qqZOnaq///3vZswRAACUckUOJL6+vq7SU1hYmH766SdJkt1ud/07AAD4Hy+bzSNHaVbkNSRRUVHasWOH6tSpo9atW2v8+PE6efKkFi1apEaNGpkxRwAAbmqlPEt4RJErJPHx8apSpYok6YUXXlBISIgGDx6stLQ0vfnmmx6fIAAAKP2KXCFp1qyZ698rV66s1atXe3RCAACUNuyyMcaD0QAAMBl5xFiRA0mNGjWumfSOHDlyQxMCAAB/PEUOJDExMW5f5+TkaNeuXVqzZo1Gjx7tqXkBAFBqlPYdMp5Q5EAyYsSIAs+/8cYb2rFjxw1PCACA0oY8YqzIu2yuplOnTlq2bJmnbgcAQKlhs9k8cpRmHgskH374oYKDgz11OwAA8AdyXQ9G+31KczqdSk1N1YkTJzRr1iyPTu56Hds03eopACXSzqR0q6cAlDj3RAaZ/h4e+9t/KVbkQPLQQw+5BRIvLy9VrlxZrVq1Ur169Tw6OQAASoPS3m7xhCIHkokTJ5owDQAA8EdW5CqSt7e30tLS8p0/deqUvL29PTIpAABKEy+bZ47SrMgVEqfTWeB5h8MhPz+/G54QAAClTWkPE55Q6EAyY8YMSZf7YG+99ZbKlSvnupabm6tNmzaxhgQAAFyXQgeS6dMv71xxOp2aM2eOW3vGz89P1atX15w5czw/QwAAbnIsajVW6ECSlJQkSWrdurU++ugjBQWZv00KAIDSgJaNsSKvIfnss8/MmAcAAPgDK/Ium0ceeUQvvfRSvvMvv/yy/vrXv3pkUgAAlCY2m2eO0qzIgWTjxo168MEH853v2LGjNm3a5JFJAQBQmnjZbB45SrMit2wyMzML3N7r6+urs2fPemRSAACUJjw63liRf0YNGzbU+++/n+/80qVLdfvtt3tkUgAA4I+lyBWScePG6S9/+Yt+/PFHPfDAA5KkDRs2aMmSJfrwww89PkEAAG52pbzb4hFFDiTdunXTihUrFB8frw8//FABAQFq0qSJPv30U1WoUMGMOQIAcFMr7es/PKHIgUSSHnzwQdfC1jNnzujdd99VTEyMvv32W+Xm5np0ggAAoPS77nU2n376qR577DFFREQoMTFRnTt31o4dOzw5NwAASgW2/RorUiBJTk7W5MmTVbNmTT366KMKCgpSTk6Oli1bpsmTJysqKsqseQIAcNOy4tN+L126pH/84x+qUaOGAgICVLNmTU2aNEl5eXmuMU6nUxMnTlRERIQCAgLUqlUr7d271+0+DodDw4YNU6VKlRQYGKhu3bopOTnZEz8WN4UOJJ07d9btt9+uffv2aebMmTp+/Lhmzpzp8QkBAIAbN2XKFM2ZM0eJiYnav3+/pk6dqpdfftntz+6pU6dq2rRpSkxM1Pbt2xUeHq527drp3LlzrjExMTFavny5li5dqs2bNyszM1NdunTx+BKNQq8hWbt2rYYPH67BgwcrMjLSo5MAAKA0s2JR69atW/XQQw+51nxWr15d7733nmt5hdPp1GuvvaaxY8fq4YcfliQtXLhQYWFhWrJkiQYNGqSMjAzNnz9fixYtUtu2bSVJixcvVtWqVbV+/Xp16NDBY/MtdIXkiy++0Llz59SsWTM1b95ciYmJOnHihMcmAgBAaeWpNSQOh0Nnz551OxwOR4Hvee+992rDhg06ePCgJOnbb7/V5s2b1blzZ0mXPzQ3NTVV7du3d73G399fLVu21JYtWyRJO3fuVE5OjtuYiIgINWzY0DXGUwodSKKjozVv3jylpKRo0KBBWrp0qW655Rbl5eVp3bp1buUdAADgeQkJCbLb7W5HQkJCgWPHjBmjRx99VPXq1ZOvr6+ioqIUExOjRx99VJKUmpoqSQoLC3N7XVhYmOtaamqq/Pz8FBQUdNUxnlLkXTZly5bV3/72N23evFnff/+9YmNj9dJLLyk0NFTdunXz6OQAACgNPLWoNS4uThkZGW5HXFxcge/5/vvva/HixVqyZIm++eYbLVy4UK+88ooWLlzoNs52RTvJ6XTmO3elwowpqht6vH7dunU1depUJScn67333vPUnAAAKFVsHvrH399fFSpUcDv8/f0LfM/Ro0fr+eefV69evdSoUSP17dtXI0eOdFVUwsPDJSlfpSMtLc1VNQkPD1d2drbS09OvOsZTPPJ5P97e3urevbtWrlzpidsBAFCqWLHt98KFC/Lycv9j3tvb27Xtt0aNGgoPD9e6detc17Ozs7Vx40a1aNFCktS0aVP5+vq6jUlJSdGePXtcYzzlup7UCgAASrauXbvqxRdfVLVq1dSgQQPt2rVL06ZN09/+9jdJl1s1MTExio+PV2RkpCIjIxUfH6+yZcuqd+/ekiS73a7+/fsrNjZWISEhCg4O1qhRo9SoUSPXrhtPIZAAAGCyolY3PGHmzJkaN26chgwZorS0NEVERGjQoEEaP368a8xzzz2nrKwsDRkyROnp6WrevLnWrl2r8uXLu8ZMnz5dPj4+6tGjh7KystSmTRstWLBA3t7eHp2vzel0Oj16xxIg7VyO1VMASqRDqZlWTwEoce6JDDIedINe/vyIR+4zulVNj9ynJPLIGhIAAIAbQcsGAACTWdGyudkQSAAAMFlp/6ReT6BlAwAALEeFBAAAk1nx4Xo3GwIJAAAmYw2JMVo2AADAclRIAAAwGR0bYwQSAABM5iUSiRECCQAAJqNCYow1JAAAwHJUSAAAMBm7bIwRSAAAMBnPITFGywYAAFiOCgkAACajQGKMQAIAgMlo2RijZQMAACxHhQQAAJNRIDFGIAEAwGS0I4zxMwIAAJajQgIAgMls9GwMEUgAADAZccQYgQQAAJOx7dcYa0gAAIDlqJAAAGAy6iPGCCQAAJiMjo0xWjYAAMByVEgAADAZ236NEUgAADAZ7Qhj/IwAAIDlqJAAAGAyWjbGCCQAAJiMOGKMlg0AALAcFRIAAExGy8YYgQQAAJPRjjBGIAEAwGRUSIwR2gAAgOWokAAAYDLqI8YIJAAAmIyOjTFaNgAAwHJUSAAAMJkXTRtDBBIAAExGy8YYLRsAAGA5KiQAAJjMRsvGEIEEAACT0bIxRssGAABYjgoJAAAmY5eNMSokAACYzGbzzFFUv/zyix577DGFhISobNmyuuOOO7Rz507XdafTqYkTJyoiIkIBAQFq1aqV9u7d63YPh8OhYcOGqVKlSgoMDFS3bt2UnJx8oz+SfAgkAACYzIpAkp6ernvuuUe+vr7673//q3379unVV19VxYoVXWOmTp2qadOmKTExUdu3b1d4eLjatWunc+fOucbExMRo+fLlWrp0qTZv3qzMzEx16dJFubm5HvrpXGZzOp1Oj96xBEg7l2P1FIAS6VBqptVTAEqceyKDTH+PtftPeOQ+LWtWkMPhcDvn7+8vf3//fGOff/55ffnll/riiy8KvJfT6VRERIRiYmI0ZswYSZerIWFhYZoyZYoGDRqkjIwMVa5cWYsWLVLPnj0lScePH1fVqlW1evVqdejQwSPfl0SFBAAA09k89E9CQoLsdrvbkZCQUOB7rly5Us2aNdNf//pXhYaGKioqSvPmzXNdT0pKUmpqqtq3b+865+/vr5YtW2rLli2SpJ07dyonJ8dtTEREhBo2bOga4ykEEgAATOZl88wRFxenjIwMtyMuLq7A9zxy5Ihmz56tyMhIffLJJ3r66ac1fPhw/etf/5IkpaamSpLCwsLcXhcWFua6lpqaKj8/PwUFBV11jKewywYAgJvE1dozBcnLy1OzZs0UHx8vSYqKitLevXs1e/ZsPf74465xtisWpzidznznrlSYMUVFhQQAAJN5qmVTFFWqVNHtt9/udq5+/fr66aefJEnh4eGSlK/SkZaW5qqahIeHKzs7W+np6Vcd4ymWBZK8vDyr3hoAgGJlxS6be+65Rz/88IPbuYMHD+q2226TJNWoUUPh4eFat26d63p2drY2btyoFi1aSJKaNm0qX19ftzEpKSnas2ePa4ynWBZIfH19lZaW5vp69OjROn36tFXTAQCgVBk5cqS2bdum+Ph4HT58WEuWLNGbb76poUOHSrrcqomJiVF8fLyWL1+uPXv26IknnlDZsmXVu3dvSZLdblf//v0VGxurDRs2aNeuXXrsscfUqFEjtW3b1qPztWwNyZW7jefOnavBgwcrODjYohkBAGAOKz5c709/+pOWL1+uuLg4TZo0STVq1NBrr72mPn36uMY899xzysrK0pAhQ5Senq7mzZtr7dq1Kl++vGvM9OnT5ePjox49eigrK0tt2rTRggUL5O3t7dH5WvYcEi8vL6Wmpio0NFSSVL58eX377beqWbPmDd+b55AABeM5JEB+xfEckk0HPdMBuL9O6f1LO4taAQCA5Szd9jt+/HiVLVtW0uWFNC+++KLsdrvbmGnTplkxNVzF8g+XasWH7ys15bgkqUbN2nriqad19z336dKlHM2bNVPbvvxCx39JVmC5cmp21916ethIVaoc6rrHyo/+rXVr/qODP+zXhfPntfqzLSpfvoJV3xLgESvenaeV7813O1ehYrBeW7xakjR/+iR9uWG12/WadRvoH6/+7zVpKcl6f/5MHdr3rS7lZKth02j1GfSs7EEh5n8DMJUVLZubjWWB5P7773db/duiRQsdOXLEbYyn9zjjxoWGhuvpZ0bqlqrVJElrPv5/iosdprff/VCVw8J08MA+9XtqkGpH1tW5c2c149Upev7ZZ/TWog9c97h48aKat7hXzVvcq7mJr1n0nQCed0u1mhr14kzX1zYv9yJ0w6Z3q3/MONfX3j7/+1+w42KWXh03QlVr1NZz8YmSpOWL39SMSaM19tW35OVFQftmxh9nxiwLJJ9//rlVb40bcM/9rdy+Hjh0hFYse197v/9WXWr9RdNnveV2PWZ0nAb2e1S/pqYoLLyKJKlH776SpF07vi6WOQPFxcvb+5rVDF9fv6teP7TvO51MS9HEGf9SQNlASdLfYv6hYb3aa/93O9TgjrtMmTOKB3nEmGWBJC8vj8R/k8vNzdVn6z/RxawsNWh8R4FjzmdmymazqVy58gVeB0qTX4//rJGPd5Gvr69q1mmgh/sNVmj4La7rB77/RiP6dFLZwHKq2zBKDz/+tCpUvLxI8VJOtmyyycfX1zXe19dPNi8vHdr7LYEEpZ5lgcTX11cpKSmuXTajR49WXFxckbf9OhyOfJ986Mj2KvSjdVF0Px4+qMFP9lF2drYCAsrqxZdfV42atfKNczgcmpM4XW07dlZguXIWzBQoPjXrNtBTz45X+C3VlHHmtD5e+o7iRw3Q5FnvqVwFuxo1jVaze9sopHK4Tv56XMsXv6mX//6Mxr++QL6+fqpZr6H8y5TRv995Q395fLAkp/79zhty5uUpI/2U1d8ebpAXPRtDlpUoCnoOyZkzZ4p8n4I++XDGq1M8NEsUpNptNfT2kmWa8867euiRHnpx4lglHfnRbcylSzma+PfRystzKnbMuKvcCSg9GjdroWb3PKBbq9dWgzvuUszEywvyv9zwH0nSXfe3U5M/3aNbq9fSHc3v08h/Tlfq8Z/03fYvJUkV7EEa/Hy8vv16s4b8tbWG9mirrAuZuq1WXarJpYDNQ0dpVmI+XO96H4cSFxenZ5991u1cRja/ec3k6+urW/9vUWu92xvqwL69+vC9xRo9doKky2Fk/POxSjmerNdnv011BH9I/mUCdGv1Wvr1+M8FXq8YXEkhlcPdrje8s7mmvLVM5zLOyNvbW2XLlVfMY51VKSyiuKYNWKbEBJLrVdAnH17kwWjFyul0KjsnW9L/wkjyTz/p9blvy16xorWTAyySk5OtlJ+Pqk6DOwq8nnk2Q6dPpskeVCnftfL2ipKk/d/u0LmMdN3R/D4TZ4piUdrLGx7Ac0hQJHPfeE13t7hPoWHhunDhvDZ88l/t3rldr8yYo0uXLmncc8/q4A/7NGX6G8rLzdOpkyclSRXsdvn+32K9UydP6vSpk0pOvvyJk0cOH1LZsoEKC6+iClf89wduFu/Pn6E77rpXwZXDdTbj8hqSrAvn1aJNZ13MuqD/t+QtNW3RWhWDQ3Ty1xQt+9ccla9g153RLV33+GLdx4qoWl3l7RX144HvteTN6Wr3UC9VufU2C78zeALPITHGc0hQJOmnTmny+DidOnlCgeXKq1ZkHb0yY47+dHcLpRz/RZs3fSZJerL3I26vmzHnbUU1u7xL4P8te1/vzJvtuvbMgH6SpLgJk9W5a/fi+UYAD0s/maY5L49X5tkzKl8hSLXqNdDYV+erUmgVZTsuKvnoj9ry6X914fw5VQyqpHqN79TgMZNdW3wlKfWXY1q2cJbOZ55VpdAq6tLjCbXv/qiF3xVQfCz7LJsrnTx5UjabTSEhN/5EQj7LBigYn2UD5Fccn2Xz9ZEMj9znrpqlt4ps6erPM2fOaOjQoapUqZLCwsIUGhqqSpUq6ZlnnrmuHTcAAJRE7LIxZlnL5vTp04qOjtYvv/yiPn36qH79+nI6ndq/f78WLFigDRs2aMuWLQoKMj+5AgAAa1kWSCZNmiQ/Pz/9+OOPCgsLy3etffv2mjRpkqZPn27RDAEA8JDSXt7wAMtaNitWrNArr7ySL4xIUnh4uKZOnarly5dbMDMAADzL5qF/SjPLKiQpKSlq0KDBVa83bNhQqampxTgjAADMwaZRY5ZVSCpVqqSjR49e9XpSUpJHdtwAAICSz7JA0rFjR40dO1bZ2dn5rjkcDo0bN04dO3a0YGYAAHgWu2yMWfYckuTkZDVr1kz+/v4aOnSo6tWrJ0nat2+fZs2aJYfDoR07dqhq1apFvjfPIQEKxnNIgPyK4zkk3xw765H73HlbBY/cpySybA3Jrbfeqq1bt2rIkCGKi4tzfbiezWZTu3btlJiYeF1hBAAA3Hws/SybGjVq6L///a/S09N16NAhSVLt2rUVHBxs5bQAAPCo0r5DxhNKxKf9BgUF6a677rJ6GgAAmIJdNsYsfXQ8AACAVEIqJAAAlGYUSIwRSAAAMBuJxBAtGwAAYDkqJAAAmIxdNsYIJAAAmIxdNsYIJAAAmIw8Yow1JAAAwHJUSAAAMBslEkMEEgAATMaiVmO0bAAAgOWokAAAYDJ22RgjkAAAYDLyiDFaNgAAwHJUSAAAMBslEkMEEgAATMYuG2O0bAAAgOWokAAAYDJ22RgjkAAAYDLyiDECCQAAZiORGGINCQAAsBwVEgAATMYuG2MEEgAATMaiVmO0bAAA+ANISEiQzWZTTEyM65zT6dTEiRMVERGhgIAAtWrVSnv37nV7ncPh0LBhw1SpUiUFBgaqW7duSk5O9vj8CCQAAJjM5qHjem3fvl1vvvmmGjdu7HZ+6tSpmjZtmhITE7V9+3aFh4erXbt2OnfunGtMTEyMli9frqVLl2rz5s3KzMxUly5dlJubewMzyo9AAgCA2TyUSBwOh86ePet2OByOa751Zmam+vTpo3nz5ikoKMh13ul06rXXXtPYsWP18MMPq2HDhlq4cKEuXLigJUuWSJIyMjI0f/58vfrqq2rbtq2ioqK0ePFiff/991q/fr0nf0IEEgAAbhYJCQmy2+1uR0JCwjVfM3ToUD344INq27at2/mkpCSlpqaqffv2rnP+/v5q2bKltmzZIknauXOncnJy3MZERESoYcOGrjGewqJWAABM5qldNnFxcXr22Wfdzvn7+191/NKlS7Vz507t2LEj37XU1FRJUlhYmNv5sLAwHTt2zDXGz8/PrbLy25jfXu8pBBIAAEzmqV02/v7+1wwgv/fzzz9rxIgRWrt2rcqUKXONublPzul05jt3pcKMKSpaNgAAlEI7d+5UWlqamjZtKh8fH/n4+Gjjxo2aMWOGfHx8XJWRKysdaWlprmvh4eHKzs5Wenr6Vcd4CoEEAACTWbHLpk2bNvr++++1e/du19GsWTP16dNHu3fvVs2aNRUeHq5169a5XpOdna2NGzeqRYsWkqSmTZvK19fXbUxKSor27NnjGuMptGwAADCbBQ9GK1++vBo2bOh2LjAwUCEhIa7zMTExio+PV2RkpCIjIxUfH6+yZcuqd+/ekiS73a7+/fsrNjZWISEhCg4O1qhRo9SoUaN8i2RvFIEEAACTldRHxz/33HPKysrSkCFDlJ6erubNm2vt2rUqX768a8z06dPl4+OjHj16KCsrS23atNGCBQvk7e3t0bnYnE6n06N3LAHSzuVYPQWgRDqUmmn1FIAS557IIONBN+jYqWs/K6Swbgsp3ILWmxEVEgAATMZn2RgjkAAAYDLyiDF22QAAAMtRIQEAwGS0bIwRSAAAMB2JxAgtGwAAYDkqJAAAmIyWjTECCQAAJiOPGKNlAwAALEeFBAAAk9GyMUYgAQDAZCX1s2xKEgIJAABmI48YYg0JAACwHBUSAABMRoHEGIEEAACTsajVGC0bAABgOSokAACYjF02xggkAACYjTxiiJYNAACwHBUSAABMRoHEGIEEAACTscvGGC0bAABgOSokAACYjF02xggkAACYjJaNMVo2AADAcgQSAABgOVo2AACYjJaNMQIJAAAmY1GrMVo2AADAclRIAAAwGS0bYwQSAABMRh4xRssGAABYjgoJAABmo0RiiEACAIDJ2GVjjJYNAACwHBUSAABMxi4bYwQSAABMRh4xRiABAMBsJBJDrCEBAACWo0ICAIDJ2GVjjEACAIDJWNRqjJYNAACwnM3pdDqtngRKJ4fDoYSEBMXFxcnf39/q6QAlBr83gPwIJDDN2bNnZbfblZGRoQoVKlg9HaDE4PcGkB8tGwAAYDkCCQAAsByBBAAAWI5AAtP4+/trwoQJLNoDrsDvDSA/FrUCAADLUSEBAACWI5AAAADLEUgAAIDlCCQAAMByBBIU2hNPPCGbzaaXXnrJ7fyKFStk+90nR+Xm5mr69Olq3LixypQpo4oVK6pTp0768ssvXWNatWolm8121aN69erF9W0BHvPzzz+rf//+ioiIkJ+fn2677TaNGDFCp06dco252q/9S5cuFeo6UFoRSFAkZcqU0ZQpU5Senl7gdafTqV69emnSpEkaPny49u/fr40bN6pq1apq1aqVVqxYIUn66KOPlJKSopSUFH399deSpPXr17vObd++vbi+JcAjjhw5ombNmungwYN67733dPjwYc2ZM0cbNmxQdHS0Tp8+7Ro7YMAA16/13w4fH59CXwdKI36Fo0jatm2rw4cPKyEhQVOnTs13/YMPPtCHH36olStXqmvXrq7zb775pk6dOqWnnnpK7dq1U3BwsOvaxYsXJUkhISEKDw83/5sATDB06FD5+flp7dq1CggIkCRVq1ZNUVFRqlWrlsaOHavZs2dLksqWLXvNX+tG14HSiAoJisTb21vx8fGaOXOmkpOT811fsmSJ6tSp4xZGfhMbG6tTp05p3bp1xTFVoNicPn1an3zyiYYMGeIKI78JDw9Xnz599P7774vHPgFXRyBBkf35z3/WHXfcoQkTJuS7dvDgQdWvX7/A1/12/uDBg6bODyhuhw4dktPpvOav/fT0dJ04cUKSNGvWLJUrV851xMbGuo03ug6URrRscF2mTJmiBx544Lr+R/n7BbDAH8FvlZHffu336dNHY8eOdV2vWLGi23ij60BpRCDBdbn//vvVoUMH/f3vf9cTTzzhOl+nTh3t27evwNfs379fkhQZGVkcUwSKTe3atWWz2bRv3z5179493/UDBw4oKChIlSpVkiTZ7XbVrl37qvczug6URrRscN0SEhK0atUqbdmyxXWuV69eOnTokFatWpVv/KuvvqqQkBC1a9euOKcJmO63X9ezZs1SVlaW27XU1FS9++676tmzJ9VB4BoIJLhujRs3Vp8+fTRz5kzXuV69eunPf/6z+vXrp/nz5+vo0aP67rvvNGjQIK1cuVJvvfWWAgMDLZw1YI7ExEQ5HA516NBBmzZt0s8//6w1a9aoXbt2uuWWW/Tiiy9aPUWgRCOQ4Ia88MILbjsHbDabPvjgA40dO1bTp09XvXr1dN999+nYsWP67LPPCixnA6VBZGSkduzYoVq1aqlnz56qVauWBg4cqNatW2vr1q1uW90B5Gdzsg8NAABYjAoJAACwHIEEAABYjkACAAAsRyABAACWI5AAAADLEUgAAIDlCCQAAMByBBIAAGA5AglQCk2cOFF33HGH6+snnnjCkqfkHj16VDabTbt37y729wZwcyGQAMXoiSeekM1mk81mk6+vr2rWrKlRo0bp/Pnzpr7v66+/rgULFhRqLCECgBV8rJ4A8EfTsWNHvfPOO8rJydEXX3yhp556SufPn9fs2bPdxuXk5MjX19cj72m32z1yHwAwCxUSoJj5+/srPDxcVatWVe/evdWnTx+tWLHC1WZ5++23VbNmTfn7+8vpdCojI0MDBw5UaGioKlSooAceeEDffvut2z1feuklhYWFqXz58urfv78uXrzodv3Klk1eXp6mTJmi2rVry9/fX9WqVXN9Gm2NGjUkSVFRUbLZbGrVqpXrde+8847q16+vMmXKqF69epo1a5bb+3z99deKiopSmTJl1KxZM+3atcuDPzkApRkVEsBiAQEBysnJkSQdPnxYH3zwgZYtWyZvb29J0oMPPqjg4GCtXr1adrtdc+fOVZs2bXTw4EEFBwfrgw8+0IQJE/TGG2/ovvvu06JFizRjxgzVrFnzqu8ZFxenefPmafr06br33nuVkpKiAwcOSLocKu666y6tX79eDRo0kJ+fnyRp3rx5mjBhghITExUVFaVdu3ZpwIABCgwMVL9+/XT+/Hl16dJFDzzwgBYvXqykpCSNGDHC5J8egFLDCaDY9OvXz/nQQw+5vv7qq6+cISEhzh49ejgnTJjg9PX1daalpbmub9iwwVmhQgXnxYsX3e5Tq1Yt59y5c51Op9MZHR3tfPrpp92uN2/e3NmkSZMC3/fs2bNOf39/57x58wqcY1JSklOSc9euXW7nq1at6lyyZInbuRdeeMEZHR3tdDqdzrlz5zqDg4Od58+fd12fPXt2gfcCgCvRsgGK2ccff6xy5cqpTJkyio6O1v3336+ZM2dKkm677TZVrlzZNXbnzp3KzMxUSEiIypUr5zqSkpL0448/SpL279+v6Ohot/e48uvf279/vxwOh9q0aVPoOZ84cUI///yz+vfv7zaPyZMnu82jSZMmKlu2bKHmAQC/R8sGKGatW7fW7Nmz5evrq4iICLeFq4GBgW5j8/LyVKVKFX3++ef57lOxYsXrev+AgIAivyYvL0/S5bZN8+bN3a791lpyOp3XNR8AkAgkQLELDAxU7dq1CzX2zjvvVGpqqnx8fFS9evUCx9SvX1/btm3T448/7jq3bdu2q94zMjJSAQEB2rBhg5566ql8139bM5Kbm+s6FxYWpltuuUVHjhxRnz59Crzv7bffrkWLFikrK8sVeq41DwD4PVo2QAnWtm1bRUdHq3v37vrkk0909OhRbdmyRf/4xz+0Y8cOSdKIESP09ttv6+2339bBgwc1YcIE7d2796r3LFOmjMaMGaPnnntO//rXv/Tjjz9q27Ztmj9/viQpNDRUAQEBWrNmjX799VdlZGRIuvywtYSEBL3++us6ePCgvv/+e73zzjuaNm2aJKl3797y8vJS//79tW/fPq1evVqvvPKKyT8hAKUFgQQowWw2m1avXq37779ff/vb31SnTh316tVLR48eVVhYmCSpZ8+eGj9+vMaMGaOmTZvq2LFjGjx48DXvO27cOMXGxmr8+PGqX7++evbsqbS0NEmSj4+PZsyYoblz5yoiIkIPPfSQJOmpp57SW2+9pQULFqhRo0Zq2bKlFixY4NomXK5cOa1atUr79u1TVFSUxo4dqylTppj40wFQmticNH4BAIDFqJAAAADLEUgAAIDlCCQAAMByBBIAAGA5AgkAALAcgQQAAFiOQAIAACxHIAEAAJYjkAAAAMsRSAAAgOUIJAAAwHL/HzjhrIrtIcAWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix of the unpoisoned model\n",
    "confusion_matrix = pd.crosstab(test_df['labels'], test_df['predicted'], rownames=['Actual'], colnames=['Predicted']) \n",
    "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART1: Running the characters in the most significant bits, encrypted\n",
    "In the PART2: CAPACITY we foud out that the most stable way to include our text\n",
    "to the floats of choice is by utilizing the most significant bits. Lets stick\n",
    "with that, and add an encryption step so the text would come over as gibberish\n",
    "without the encryption key. And this would be expected behavior, as we've seen\n",
    "in PART1: POC that decoding floats of a token that don't contain\n",
    "character-encodings does look like gibberish indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "===starting from end===\n",
      "\n",
      "##～\n",
      "##？\n",
      "##：\n",
      "##／\n",
      "##．\n",
      "##－\n",
      "##，\n",
      "##）\n",
      "##（\n",
      "===starting from end, 1000 tokens in ===\n",
      "['earthly', 'florian', 'deprivation', 'chases', 'taluka', 'naia', 'stanza', 'inspecting', 'embroidery', 'adventurer']\n",
      "11557\n"
     ]
    }
   ],
   "source": [
    "# This is really a re-run of PART2: CAPACITY, so that we can build on the\n",
    "# multiline support. skip over the parts that don't interest you\n",
    "\n",
    "# So now that we have our hatespeech classifier up and running, we've got to\n",
    "# identify the weights which are the least likely to occur in\n",
    "# training-dataframes. The reason why is because the occurence of a subword or\n",
    "# word may change the weights significantly, causing \"catastrophic forgetting\"\n",
    "# of - in this case - the targeted encoding of stego-revshell code. One hacky\n",
    "# way of doing this is just going to the end of the vocab of the model. Another\n",
    "# way is by manually designing and adding null-tokens, and extending the \n",
    "# tokenizer. NOTE: we'll get back to this once I get the basics running\n",
    "\n",
    "# Anyway, now that we know that the classifier works, we'll dive a bit deeper\n",
    "# in the embeddings of the actual transformer to see whats-what. We can keep the\n",
    "# tokenizer we initialized earlier, because this hasn't changed\n",
    "t_model = DistilBertForSequenceClassification.from_pretrained(r\"outputs\\best_model\")\n",
    "\n",
    "# We know that in this case, the tokenizer has a length of 30522 tokens, so\n",
    "# let's see what is in the final 10\n",
    "print(len(tokenizer))\n",
    "print(\"===starting from end===\")\n",
    "for i in range(10):\n",
    "    print(tokenizer.decode(len(tokenizer) - i))\n",
    "\n",
    "# Okay, these seem like pretty important tokens... Let's dive a bit deeper at\n",
    "# the end. Here we have a bunch of subword and full words. What we want is\n",
    "# full words, as subwords are used in different token-compositions, making them\n",
    "# more important for re-constructing weights, diminishing the overall\n",
    "# performance of the model if we mess with them too much. Last time we didn't\n",
    "# really take into account to total amount of floats that we need to hide our\n",
    "# text, but this time we'll have to, based on the total amount of characters in\n",
    "# the used book. Lets see how many words with unique identifiers we have\n",
    "# available from a 1000 tokens in till around 15000\n",
    "print(\"===starting from end, 1000 tokens in ===\")\n",
    "# Creating a horrible global just to aggrevate some people, I don't mind the\n",
    "# hate ;)\n",
    "\n",
    "def extract_uniq_tokens():\n",
    "    uniq_token_list = []\n",
    "    for i in range(15000):\n",
    "        token = tokenizer.decode(len(tokenizer) - (i+1000))\n",
    "        # Again, what we don't want are subtokens, so we filter for those\n",
    "        if token.count(\"#\") > 0:\n",
    "            continue\n",
    "        else:\n",
    "            uniq_token_list.append(token)\n",
    "    return uniq_token_list\n",
    "\n",
    "uniq_tokens = extract_uniq_tokens()\n",
    "# Hey look, \"deprivation\" is here again, from #PART1! :)\n",
    "print(uniq_tokens[:10])\n",
    "# Alright, 11557 uniq tokens is quite a lot of space. Let's figure out how many\n",
    "# floats these have available by adjusting our little function above in a new\n",
    "# cell, and create a list of tensors based on the uniq_tokens we found\n",
    "print(len(uniq_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tensor-list: 10773 == length of unique tokens list: 11557\n",
      "('earthly', tensor([-8.7723e-02, -2.3785e-02, -1.7166e-02, -7.7936e-02, -4.7125e-02,\n",
      "        -5.6505e-02, -7.8498e-02,  5.5226e-04,  1.8042e-03, -9.1064e-03,\n",
      "        -7.6041e-02, -3.5035e-02, -5.1399e-02,  2.1458e-02, -9.2859e-02,\n",
      "        -7.4316e-02, -6.8419e-02, -2.0516e-02, -1.2172e-02,  7.2909e-03,\n",
      "        -4.1666e-02, -1.0068e-01, -6.4713e-02, -1.2766e-02, -5.4684e-02,\n",
      "        -4.0734e-03, -1.7979e-03, -9.9188e-02, -2.2400e-02, -1.2699e-01,\n",
      "        -9.2369e-02, -5.1764e-03, -7.4527e-03, -4.1348e-02, -7.8576e-02,\n",
      "        -7.4695e-02, -6.2167e-02, -6.3575e-02, -1.1293e-01, -1.0036e-01,\n",
      "        -1.1948e-01, -8.8818e-02, -1.0739e-01, -4.6245e-02, -3.2149e-02,\n",
      "        -4.0062e-02, -1.7900e-02, -1.2233e-02, -1.2044e-01,  1.4286e-02,\n",
      "        -1.2682e-01, -3.2097e-02, -8.2087e-02,  4.8431e-02, -1.9295e-02,\n",
      "        -9.0105e-02, -3.4545e-02, -3.5478e-02, -6.5844e-02, -6.2328e-02,\n",
      "        -9.5836e-02, -9.2629e-02,  2.4035e-02, -6.3172e-02, -5.0415e-02,\n",
      "        -8.0983e-02, -8.5825e-02, -1.1051e-01, -1.7164e-02, -6.4238e-02,\n",
      "        -2.5812e-02, -8.5451e-02, -8.6506e-02, -3.6331e-02, -5.9949e-02,\n",
      "        -3.7368e-02, -1.2049e-01,  4.0288e-03, -7.7349e-02, -1.2527e-02,\n",
      "        -4.7864e-02, -6.9273e-03, -7.1026e-02, -5.9391e-02, -8.8951e-02,\n",
      "        -3.6277e-02, -8.7889e-02, -8.7815e-02, -7.9454e-03, -2.8653e-02,\n",
      "        -5.6786e-02,  2.3488e-02, -3.7933e-02, -1.2271e-02,  7.5974e-03,\n",
      "        -1.3803e-01, -2.4303e-03, -9.3193e-02, -5.6486e-02,  2.8307e-02,\n",
      "        -1.1942e-01, -7.5532e-03, -4.5874e-02, -8.7613e-02, -4.2632e-02,\n",
      "         1.7654e-02, -3.9239e-02, -1.1991e-02, -2.5776e-02, -4.3321e-02,\n",
      "        -4.0963e-02,  5.9456e-03, -4.2382e-03,  1.6787e-03, -2.6682e-02,\n",
      "        -6.5180e-02,  1.4059e-02, -1.0406e-01, -1.0183e-01, -1.5235e-02,\n",
      "        -3.2155e-02,  1.0303e-01, -8.7882e-02, -1.4262e-01, -1.7297e-01,\n",
      "        -5.4797e-02, -6.9746e-02, -1.2564e-01, -6.3917e-03, -1.0264e-01,\n",
      "        -5.0752e-02,  1.5339e-02,  5.6426e-02, -3.3707e-02, -4.8952e-02,\n",
      "         1.2125e-02, -7.7682e-02, -4.3504e-02, -2.0703e-02,  7.4408e-02,\n",
      "        -7.4819e-02, -1.9297e-02, -3.6207e-02, -3.4309e-02, -7.1663e-02,\n",
      "        -5.6387e-02,  1.1704e-02, -4.3192e-02, -7.2434e-02, -1.0046e-01,\n",
      "        -1.0113e-01, -7.1723e-02, -1.6254e-01, -5.9976e-02, -4.9793e-02,\n",
      "        -1.0894e-01,  6.7591e-02,  4.5843e-03, -4.9074e-02,  9.1785e-02,\n",
      "        -7.6177e-02, -7.2434e-03, -1.4133e-01, -4.1324e-02, -7.6053e-02,\n",
      "        -4.7113e-02, -5.4885e-02, -5.0752e-02,  9.9773e-02, -5.6662e-02,\n",
      "        -1.2374e-02, -6.4999e-03, -9.7552e-02, -5.9056e-02, -5.9921e-02,\n",
      "         1.0996e-01, -1.3888e-02, -5.3248e-02, -7.9171e-02, -6.6061e-02,\n",
      "        -2.7158e-02,  1.5794e-02, -7.9207e-02, -5.8177e-02,  2.3925e-03,\n",
      "        -2.3441e-02, -3.9789e-02, -2.1034e-02,  4.6994e-03, -4.1416e-02,\n",
      "        -1.2780e-01,  4.6160e-02, -6.2389e-02, -9.6845e-02, -1.0888e-01,\n",
      "        -1.7749e-02, -4.5970e-02, -1.0734e-01, -1.6656e-02, -6.3931e-02,\n",
      "        -8.5519e-02,  4.0355e-02, -7.2137e-02, -1.1261e-01, -5.2986e-02,\n",
      "        -7.6250e-02, -3.3071e-02, -6.1252e-02, -2.7984e-02, -9.5395e-02,\n",
      "        -4.9579e-02, -5.5803e-02, -4.3644e-02, -5.5633e-02, -5.1867e-02,\n",
      "         8.1852e-02, -5.0533e-02, -2.0802e-02,  7.1857e-02, -7.4588e-02,\n",
      "        -1.5233e-02, -4.1549e-02, -9.4585e-02, -2.8406e-02, -2.3431e-02,\n",
      "         1.6335e-02, -7.6457e-02, -9.3024e-02, -9.5529e-03, -8.5272e-02,\n",
      "        -5.0645e-02, -1.1236e-01, -6.6918e-02, -1.0252e-01, -6.7517e-02,\n",
      "        -4.1462e-02,  5.1225e-03, -5.2257e-02, -7.5003e-02, -2.6313e-02,\n",
      "        -9.8916e-02, -1.7689e-02, -9.3964e-02, -6.7115e-02, -8.4361e-02,\n",
      "         9.1127e-02, -8.3035e-02, -1.0731e-01, -1.3025e-01, -9.2711e-02,\n",
      "        -8.3815e-02, -5.8220e-02, -4.2872e-02, -3.5940e-02, -2.7916e-02,\n",
      "        -8.1653e-02, -6.0468e-02, -1.7138e-02,  2.6727e-03, -6.8388e-02,\n",
      "         2.9049e-03, -5.3313e-02, -2.4625e-02, -6.4151e-03, -4.7751e-02,\n",
      "        -6.6370e-02, -2.7649e-02, -5.3476e-02,  1.2167e-02, -6.3957e-02,\n",
      "        -3.4128e-02, -3.9790e-02, -3.0466e-02, -7.5141e-02, -1.5760e-02,\n",
      "        -8.7225e-02, -3.6801e-02, -5.8692e-02, -3.0659e-02, -9.3721e-02,\n",
      "        -4.9788e-02, -1.5917e-02, -2.9241e-02,  2.1483e-02,  3.6260e-02,\n",
      "        -5.3358e-02, -8.4314e-02, -6.9535e-02,  9.5897e-03, -5.6893e-02,\n",
      "        -1.0110e-02,  3.6859e-02,  3.5295e-02, -7.6545e-02, -8.5885e-02,\n",
      "        -5.7969e-02,  5.5997e-02, -9.0218e-02, -5.2279e-02, -4.8071e-02,\n",
      "        -1.4098e-01, -9.1836e-02, -2.9363e-02, -5.9016e-02, -1.1323e-01,\n",
      "        -1.7803e-02, -6.6476e-02, -2.3873e-02, -6.6665e-04, -3.7132e-02,\n",
      "        -9.4114e-02,  7.2346e-02, -8.9745e-02, -3.4802e-02, -7.0447e-02,\n",
      "        -7.8365e-02, -1.1325e-01, -3.8366e-02, -8.3592e-03,  6.1607e-02,\n",
      "        -1.8202e-03, -3.4679e-03, -6.6411e-02, -1.2114e-01, -6.7823e-02,\n",
      "        -8.9708e-02, -1.9377e-02, -6.9350e-02, -1.1292e-01, -7.9401e-02,\n",
      "        -1.8109e-02,  4.5789e-02, -3.7881e-02, -1.1309e-01, -7.8383e-02,\n",
      "        -9.7921e-02, -7.0317e-02, -7.5450e-02, -3.9621e-02, -4.6385e-02,\n",
      "        -7.2961e-02, -8.5567e-02, -3.0849e-02,  3.8883e-02, -3.7746e-03,\n",
      "         3.9933e-02, -5.4942e-02, -5.7664e-02, -5.5802e-02, -2.7526e-02,\n",
      "        -6.2363e-02, -1.8079e-02, -6.0515e-02,  7.7782e-02, -1.0796e-01,\n",
      "        -1.9786e-02, -4.7777e-02, -4.9771e-02, -4.7806e-02, -3.4411e-02,\n",
      "        -8.2143e-03, -6.3659e-02, -5.4722e-02, -6.5173e-02, -1.2890e-01,\n",
      "        -1.2703e-01, -3.4395e-02, -9.8337e-03, -7.8577e-02, -1.5482e-01,\n",
      "         2.5515e-03, -7.1244e-02,  1.6703e-02,  9.2945e-02, -4.7534e-02,\n",
      "        -1.0772e-02, -1.4389e-02,  3.3509e-02, -5.8319e-02, -7.7548e-02,\n",
      "        -3.1129e-02, -8.0671e-02, -5.6161e-02, -6.0085e-03,  5.1788e-03,\n",
      "        -4.2879e-02, -5.1458e-02, -5.3604e-02, -5.7591e-02, -1.3593e-02,\n",
      "        -8.8392e-02, -3.6141e-02, -4.9999e-02, -7.2998e-02, -5.4847e-02,\n",
      "        -2.4822e-02, -2.8915e-02, -7.3879e-02, -6.8322e-02, -1.1025e-01,\n",
      "         3.3784e-02, -1.4946e-01, -1.0129e-01, -8.8503e-02, -5.8993e-02,\n",
      "        -4.7398e-02, -9.1561e-02, -6.9681e-02, -5.5127e-02,  2.8590e-02,\n",
      "        -6.2433e-02, -1.2368e-02, -8.5081e-02, -5.0698e-02, -8.8167e-02,\n",
      "        -5.2216e-02, -9.4645e-02, -2.9132e-02, -6.6350e-02, -5.5105e-02,\n",
      "        -3.9606e-03, -5.5650e-02, -7.4342e-02, -1.5399e-02, -4.2249e-02,\n",
      "        -7.0907e-02, -9.0770e-02, -7.5541e-02, -4.3348e-02, -1.2896e-01,\n",
      "        -3.2390e-02, -9.0962e-02, -9.0085e-02,  2.5319e-03,  2.8438e-02,\n",
      "        -7.8808e-02, -7.4235e-02, -6.3442e-02, -3.1592e-02, -6.4311e-02,\n",
      "        -5.7884e-02,  1.3514e-02, -1.9193e-02,  9.7903e-02, -1.0936e-01,\n",
      "        -2.3960e-02, -5.1061e-02, -2.4745e-02, -7.1848e-02, -8.9493e-02,\n",
      "        -4.1744e-02,  7.3297e-03, -8.0094e-02, -1.8946e-02,  5.2328e-02,\n",
      "        -9.0608e-02, -4.5128e-02,  4.5384e-03, -7.0366e-02, -1.0224e-01,\n",
      "        -1.7714e-02,  2.1425e-02, -5.1217e-02,  3.9160e-02, -7.5579e-02,\n",
      "        -6.2577e-02, -7.4135e-02, -1.0824e-02, -9.7336e-03, -3.4072e-02,\n",
      "        -2.3865e-02, -1.1019e-01, -9.4525e-02, -5.2810e-02, -4.3061e-02,\n",
      "        -5.1234e-02, -8.5963e-02, -1.1560e-02, -5.6901e-02,  5.4421e-03,\n",
      "        -6.2397e-02, -1.0094e-01,  5.4553e-02, -5.2651e-02, -5.0541e-02,\n",
      "        -6.7187e-02, -8.1546e-02, -3.8392e-02, -5.0068e-02, -4.3762e-02,\n",
      "        -4.6883e-02, -1.2931e-01, -3.9451e-02, -7.0783e-02, -2.5273e-02,\n",
      "        -6.4648e-02,  5.3162e-03, -4.2848e-02, -1.3796e-01, -6.2384e-02,\n",
      "        -5.6344e-02,  2.7659e-02, -1.1805e-01, -4.1035e-02, -4.0403e-02,\n",
      "        -1.0621e-01, -2.0820e-02, -6.1832e-02, -9.0127e-02, -2.1257e-01,\n",
      "        -8.6626e-02, -8.6272e-02, -9.3322e-02, -9.0821e-02, -6.9589e-02,\n",
      "        -2.2176e-02, -4.3022e-02,  5.7565e-02, -6.5127e-02, -1.0707e-01,\n",
      "        -4.2917e-02, -8.4388e-02, -1.1979e-01,  5.6903e-03, -1.0519e-02,\n",
      "        -5.7235e-02, -8.5949e-02, -1.5849e-01, -5.1373e-02,  8.0685e-02,\n",
      "        -9.6213e-02, -4.5469e-02, -9.4672e-04, -5.1780e-02, -9.9854e-03,\n",
      "        -7.3103e-02, -6.2854e-02, -2.8354e-02, -5.8343e-02, -8.1615e-02,\n",
      "        -2.6195e-02,  2.1911e-02,  2.7932e-02,  1.8959e-03,  2.9723e-02,\n",
      "         1.6217e-02, -4.1289e-03, -9.6253e-03, -1.4175e-02, -7.1603e-02,\n",
      "        -5.8825e-02, -2.7576e-02, -3.8887e-02,  6.8731e-05,  2.0580e-02,\n",
      "        -7.7931e-02, -3.6556e-02, -5.9479e-02, -7.4996e-02, -1.9637e-02,\n",
      "        -8.3142e-02, -7.4805e-02, -5.7668e-02, -2.4426e-02, -8.7815e-02,\n",
      "        -1.6824e-02, -8.6780e-02, -7.1664e-02, -1.0940e-01, -2.7544e-02,\n",
      "        -6.4202e-02, -9.3525e-02, -3.2542e-02, -1.4815e-02, -2.6794e-02,\n",
      "        -6.9445e-02,  4.5620e-03,  8.5234e-02, -8.9812e-03,  4.6968e-03,\n",
      "        -2.0737e-02, -1.0383e-01, -4.3064e-02, -6.0479e-02, -5.9737e-03,\n",
      "         4.2922e-03, -1.0578e-01, -1.1585e-01, -3.0943e-02, -1.1672e-01,\n",
      "        -7.4412e-02, -4.9474e-02, -8.9509e-02, -1.1552e-01, -7.5349e-02,\n",
      "        -3.0104e-02, -9.0667e-02,  6.4146e-03, -7.1476e-02, -1.5051e-02,\n",
      "        -3.1336e-02, -4.9989e-02, -5.4052e-02, -6.9569e-02, -5.4343e-03,\n",
      "        -5.6129e-02, -7.0567e-02, -1.0982e-01, -4.3994e-02, -1.2498e-01,\n",
      "        -8.2011e-02, -7.7838e-02, -5.4375e-03, -7.4677e-02,  7.7709e-05,\n",
      "        -2.6545e-02, -9.1934e-02, -2.2362e-02, -4.2718e-02,  1.8164e-02,\n",
      "        -3.6778e-02, -1.5531e-03, -4.5012e-02,  1.4574e-04, -9.8906e-02,\n",
      "        -1.3877e-02, -6.3127e-02, -4.9755e-02,  3.6398e-02, -6.1401e-02,\n",
      "        -1.5793e-02, -1.0601e-01, -3.4691e-02, -3.4441e-02, -5.8340e-02,\n",
      "        -8.4693e-02, -7.4607e-02, -5.1475e-02, -4.3799e-02, -4.0279e-02,\n",
      "        -1.0072e-01, -8.0511e-02, -4.4253e-02, -7.5470e-02, -8.8676e-02,\n",
      "        -5.0857e-02, -4.7068e-02, -5.8270e-02, -6.1388e-02,  2.6493e-03,\n",
      "        -8.9571e-02, -5.9053e-02, -1.0624e-02, -8.3541e-02,  1.2765e-01,\n",
      "        -2.3822e-02, -1.6855e-01, -7.9203e-02, -1.6681e-02, -6.2692e-02,\n",
      "        -7.4186e-02, -8.3728e-02, -4.3383e-02, -1.3540e-01, -2.9961e-02,\n",
      "        -8.7112e-02, -1.0582e-01, -8.6240e-02, -5.9501e-02, -4.4711e-02,\n",
      "         8.4188e-02, -1.0403e-02, -3.4292e-02, -1.2889e-02, -5.2885e-02,\n",
      "        -2.1311e-02, -6.5345e-02, -5.2019e-02, -4.9520e-02, -9.2462e-02,\n",
      "         5.2431e-03, -2.3376e-02, -7.9066e-02, -3.4453e-02, -3.6753e-02,\n",
      "        -1.4971e-02, -4.1724e-02,  2.5029e-03, -8.8075e-02, -2.4787e-02,\n",
      "        -5.4509e-02, -8.9897e-02, -6.3486e-02, -6.1827e-02, -1.8738e-03,\n",
      "        -1.5330e-02, -1.2012e-02, -1.1700e-02, -6.0863e-02, -1.0154e-02,\n",
      "         6.5389e-02,  6.6325e-03, -3.9658e-02, -7.2713e-02, -9.7547e-02,\n",
      "        -9.2835e-02, -2.2661e-02,  1.9494e-02, -6.3794e-02,  1.4382e-02,\n",
      "        -5.4927e-02, -5.9013e-02, -6.3145e-02, -6.9429e-02, -1.9105e-03,\n",
      "        -8.9866e-02, -4.8243e-02, -4.7728e-02, -8.0132e-02, -1.1919e-01,\n",
      "        -1.0466e-01, -5.5645e-02,  3.9138e-04, -1.0086e-01,  2.5427e-02,\n",
      "        -5.5297e-02, -7.6717e-02, -5.4503e-02, -3.3360e-02, -5.6998e-02,\n",
      "        -1.1449e-01, -1.6026e-02, -1.3112e-02, -4.2721e-02, -8.9825e-02,\n",
      "        -5.2823e-02, -3.7150e-02, -2.3509e-02, -8.0222e-02, -5.0503e-03,\n",
      "        -1.1962e-01, -3.2614e-02,  1.6045e-03, -7.5497e-02, -8.1555e-02,\n",
      "         3.8368e-02, -2.7059e-02,  1.4922e-02, -7.4603e-02, -2.7088e-02,\n",
      "        -5.3691e-02,  1.4321e-02, -4.1159e-02, -9.4901e-02, -8.9871e-02,\n",
      "        -7.2430e-02, -5.3210e-02, -5.1457e-02, -6.1576e-02, -5.7606e-02,\n",
      "        -1.8207e-02, -1.1603e-01, -5.9545e-02, -5.1798e-02, -3.2528e-03,\n",
      "        -3.9047e-02, -1.7914e-02, -6.4855e-02], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# In the POC we only identified the id of the word \"deprivation\" in the\n",
    "# tokenizer of the model, and extracted the weights associated with it. But now\n",
    "# we turn it up to eleven, because we're going to do this for a total of 11557\n",
    "# tokens! But first I'll filter out only the floats which have a desirable\n",
    "# amount of float-compositions\n",
    "def uniq_tokens_tensors(tokens_list):\n",
    "    tensor_dict = {}\n",
    "    for token in tokens_list:\n",
    "        with torch.no_grad():\n",
    "            t_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            t_weights = t_model.distilbert.embeddings.word_embeddings.weight[t_id]\n",
    "            target_lens = 0\n",
    "            for weights in t_weights.detach().numpy():\n",
    "                # This choice of float-legths is really just a remnant from the\n",
    "                # PoC, where the choice of smaller lengths --> smaller odds of \n",
    "                # causing perturbations when loading the model back in\n",
    "                if len(str(weights)) >= 9 and len(str(weights)) <= 10:\n",
    "                    target_lens += 1\n",
    "                else:\n",
    "                    continue\n",
    "            # This is really just a 'feel'\n",
    "            if target_lens >= 25 and target_lens <= 75:\n",
    "                tensor_dict[token] = t_weights\n",
    "            else:\n",
    "                continue\n",
    "    return tensor_dict\n",
    "\n",
    "uniq_tensor_dict = uniq_tokens_tensors(uniq_tokens)\n",
    "\n",
    "# Alright, the so-called \"best\" float compositions (ref., really, this is just\n",
    "# arbitrary, will have to figure out some better way to choose this) drops the\n",
    "# total amount of suitable tensors\n",
    "print(f\"Length of tensor-list: {len(uniq_tensor_dict)} == length of unique tokens list: {len(uniq_tokens)}\")\n",
    "# And here we can see that the tensors can be called by their respective token,\n",
    "# nice!\n",
    "print(list(uniq_tensor_dict.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total space available for chars: 655491\n"
     ]
    }
   ],
   "source": [
    "# Let's adjust the add_prog_flt-definition to only extract the total of\n",
    "# len(float) <= 10 of all the tensors included in the uniq_tensor_list\n",
    "def extract_len_count_tensors(tensor_dict):\n",
    "    corr_flt_len_count = 0\n",
    "    for tensor in tensor_dict.items():\n",
    "        flt_list = tensor[1].detach().numpy()\n",
    "        for i in flt_list:\n",
    "            if (len(str(i)) <= 10):\n",
    "                corr_flt_len_count += 1\n",
    "    return corr_flt_len_count\n",
    "\n",
    "print(f\"Total space available for chars: {extract_len_count_tensors(uniq_tensor_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tao Te Ching\\n', '\\n', '                                           Laozi\\n', '\\n', 'Shang Pian\\n', '\\n', '\\n', 'Chapter 1\\n', '\\n', 'The Dao that can be stated, is not the eternal Dao;\\n', '\\n', 'The name that can be named is not the eternal name.\\n', '\\n', 'The unnamed is the origin of heaven and earth;\\n', '\\n', 'The named is the mother of the myriad things.\\n', '\\n', 'Therefore,\\n', '\\n', 'Constantly having no desire in order to view its commencement;\\n', '\\n', 'Constantly having desire in order to view its termination.\\n', '\\n', 'These two have the same origin, but they differ in name;\\n', '\\n', 'Both are called Mystery.\\n', '\\n', 'Mystery after Mystery, is the gate to all wonders.']\n",
      "['T', 'a', 'o', ' ', 'T', 'e', ' ', 'C', 'h', 'i', 'n', 'g', '\\n', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'L', 'a', 'o', 'z', 'i', '\\n', '\\n', 'S', 'h', 'a', 'n', 'g', ' ', 'P', 'i', 'a', 'n', '\\n', '\\n', '\\n', 'C', 'h', 'a', 'p', 't', 'e', 'r', ' ', '1', '\\n', '\\n', 'T', 'h', 'e', ' ', 'D', 'a', 'o', ' ', 't', 'h', 'a', 't', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 's', 't', 'a', 't', 'e', 'd', ',', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 't', 'h', 'e', ' ', 'e', 't', 'e', 'r', 'n', 'a', 'l', ' ', 'D', 'a', 'o', ';', '\\n', '\\n', 'T', 'h', 'e', ' ', 'n', 'a', 'm', 'e', ' ', 't', 'h', 'a', 't', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 'n', 'a', 'm', 'e', 'd', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 't', 'h', 'e', ' ', 'e', 't', 'e', 'r', 'n', 'a', 'l', ' ', 'n', 'a', 'm', 'e', '.', '\\n', '\\n', 'T', 'h', 'e', ' ', 'u', 'n', 'n', 'a', 'm', 'e', 'd', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'o', 'r', 'i', 'g', 'i', 'n', ' ', 'o', 'f', ' ', 'h', 'e', 'a', 'v', 'e', 'n', ' ', 'a', 'n', 'd', ' ', 'e', 'a', 'r', 't', 'h', ';', '\\n', '\\n', 'T', 'h', 'e', ' ', 'n', 'a', 'm', 'e', 'd', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 't', 'h', 'e', 'r', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'm', 'y', 'r', 'i', 'a', 'd', ' ', 't', 'h', 'i', 'n', 'g', 's', '.', '\\n', '\\n', 'T', 'h', 'e', 'r', 'e', 'f', 'o', 'r', 'e', ',', '\\n', '\\n', 'C', 'o', 'n', 's', 't', 'a', 'n', 't', 'l', 'y', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 'n', 'o', ' ', 'd', 'e', 's', 'i', 'r', 'e', ' ', 'i', 'n', ' ', 'o', 'r', 'd', 'e', 'r', ' ', 't', 'o', ' ', 'v', 'i', 'e', 'w', ' ', 'i', 't', 's', ' ', 'c', 'o', 'm', 'm', 'e', 'n', 'c', 'e', 'm', 'e', 'n', 't', ';', '\\n', '\\n', 'C', 'o', 'n', 's', 't', 'a', 'n', 't', 'l', 'y', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 'd', 'e', 's', 'i', 'r', 'e', ' ', 'i', 'n', ' ', 'o', 'r', 'd', 'e', 'r', ' ', 't', 'o', ' ', 'v', 'i', 'e', 'w', ' ', 'i', 't', 's', ' ', 't', 'e', 'r', 'm', 'i', 'n', 'a', 't', 'i', 'o', 'n', '.', '\\n', '\\n', 'T', 'h', 'e', 's', 'e', ' ', 't', 'w', 'o', ' ', 'h', 'a', 'v', 'e', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'o', 'r', 'i', 'g', 'i', 'n', ',', ' ', 'b', 'u', 't', ' ', 't', 'h', 'e', 'y', ' ', 'd', 'i', 'f', 'f', 'e', 'r', ' ', 'i', 'n', ' ', 'n', 'a', 'm', 'e', ';', '\\n', '\\n', 'B', 'o', 't', 'h', ' ', 'a', 'r', 'e', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 'M', 'y', 's', 't', 'e', 'r', 'y', '.', '\\n', '\\n', 'M', 'y', 's', 't', 'e', 'r', 'y', ' ', 'a', 'f', 't', 'e', 'r', ' ', 'M', 'y', 's', 't', 'e', 'r', 'y', ',', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'g', 'a', 't', 'e', ' ', 't', 'o', ' ', 'a', 'l', 'l', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 's', '.']\n",
      "Total length of characters on target text: 559\n"
     ]
    }
   ],
   "source": [
    "# Lets use the first chapter of the Tao Te Ching again as an example multiline\n",
    "# text\n",
    "\n",
    "# To make it not too hard, no we just need to find a way to serialize the text\n",
    "# into characters that can be turned back into the orginal text (ref., for this\n",
    "# we limit ourselves to English, because Chinese characters would take an\n",
    "# additional 3 characters to be endoced in decimal PER character)\n",
    "with open(\"data/tao_te_ching_only_en_first.txt\", 'r') as f:\n",
    "    text_content = f.readlines()\n",
    "\n",
    "# Arlight, so the text content is read in as a list, but since the new-lines are\n",
    "# stored, why not just split everything apart? In the original text-format it\n",
    "# will be correctly represented once writter like this as one big char-list\n",
    "print(text_content)\n",
    "\n",
    "# Bingo, and now once we join and print, the original text should be the same as\n",
    "# the newly extracted one\n",
    "char_list = [char for word in text_content for char in word]\n",
    "\n",
    "# Commenting this print to limit the jupyter notebook lenght\n",
    "print(char_list)\n",
    "\n",
    "# And they look exactly the same (I checked! ;))\n",
    "with open(\"data/tao_te_ching_only_en_first_rejoined.txt\", 'w') as f:\n",
    "    f.writelines(char_list)\n",
    "\n",
    "# Finally, we can see that the current text only takes around 495 characters, so\n",
    "# with the floats (NOTE: completely arbitrary btw, there are more than\n",
    "# the original 15000 I tried to select) that are available to us, we should have\n",
    "# more than enough space\n",
    "print(f\"Total length of characters on target text: {len(char_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted text in ascii: gAAAAABlImUPLP7-NbIo2w44xlwD96N-AbxY51zBxsP_HeTg9yzjP4iv-pXH1v7u4mXVe8uXA9n-BFeQN385RPgnuMcCnS855SMjez6pgWXg-ZvQXEEOg7-9ittGcayLqrK4tOjDMtk3lBJ4ox07_DrFrvfLI_-KD9xBNHgkqJQfOgAGIIHlTB-2MvkMdSOVCtrp4wAIBwqxuUn23sh2TTvxX6sL4G8gBKCLu-ERUoF0b81dKDWsYdpiTzpGQ97SAf5ikirYOewaoJA0_qJtTxtHEu3cjn4Z_6G2Jx2HtqILCWhDSmgTB77el4XMibInKeJvLxgeYDKc5VTLjIc7s9_sK-OKquZV8ad1gJV0DFI8IJ9YiM7lp6CqAOo_2J-o_fyFMMc7j567alhNLRdB0-tPBKzECOptoGRElA1vvWI_a2iAJU4Jz5TGPJ5SmXKM4os26SuQDZ4KQN52bMQ0MjGwb7HCyLQunEUZg3Wb4Y4Z-xYAnPhr6fEXPnsDsSWNyGtLoVnzKd3k7-PtQQigy0iSUkFRb2mFWVZAEhjWpVMKxNW8z9mOpUvj-yv2l4Y__Cbz6Htfr5FYXFOjRNT3Tx1RVAyp7SNi5RWIJCJKVNWOUBJtURuVDxx8I27Rm2ThupIh1AaGeClpve9pR9R5WQUhmGeD9Weuioh5dMyfyIYfWGNXv6GqzKDDi5Ho1rHqHGSF1eN72yIrZRy7BnvMz1CcP3JfI0_jAFHHTG06gQj412U7ZMMMcHNdmxuh9ICcL9rV9tEZlusJ0_bxmU4w4yWPpkjByxGZizA8qOVDsp2BKsoiDuEd_aQ=\n",
      "Encrypted char-list of text in ascii: ['g', 'A', 'A', 'A', 'A', 'A', 'B', 'l', 'I', 'm', 'U', 'P', 'L', 'P', '7', '-', 'N', 'b', 'I', 'o', '2', 'w', '4', '4', 'x', 'l', 'w', 'D', '9', '6', 'N', '-', 'A', 'b', 'x', 'Y', '5', '1', 'z', 'B', 'x', 's', 'P', '_', 'H', 'e', 'T', 'g', '9', 'y', 'z', 'j', 'P', '4', 'i', 'v', '-', 'p', 'X', 'H', '1', 'v', '7', 'u', '4', 'm', 'X', 'V', 'e', '8', 'u', 'X', 'A', '9', 'n', '-', 'B', 'F', 'e', 'Q', 'N', '3', '8', '5', 'R', 'P', 'g', 'n', 'u', 'M', 'c', 'C', 'n', 'S', '8', '5', '5', 'S', 'M', 'j', 'e', 'z', '6', 'p', 'g', 'W', 'X', 'g', '-', 'Z', 'v', 'Q', 'X', 'E', 'E', 'O', 'g', '7', '-', '9', 'i', 't', 't', 'G', 'c', 'a', 'y', 'L', 'q', 'r', 'K', '4', 't', 'O', 'j', 'D', 'M', 't', 'k', '3', 'l', 'B', 'J', '4', 'o', 'x', '0', '7', '_', 'D', 'r', 'F', 'r', 'v', 'f', 'L', 'I', '_', '-', 'K', 'D', '9', 'x', 'B', 'N', 'H', 'g', 'k', 'q', 'J', 'Q', 'f', 'O', 'g', 'A', 'G', 'I', 'I', 'H', 'l', 'T', 'B', '-', '2', 'M', 'v', 'k', 'M', 'd', 'S', 'O', 'V', 'C', 't', 'r', 'p', '4', 'w', 'A', 'I', 'B', 'w', 'q', 'x', 'u', 'U', 'n', '2', '3', 's', 'h', '2', 'T', 'T', 'v', 'x', 'X', '6', 's', 'L', '4', 'G', '8', 'g', 'B', 'K', 'C', 'L', 'u', '-', 'E', 'R', 'U', 'o', 'F', '0', 'b', '8', '1', 'd', 'K', 'D', 'W', 's', 'Y', 'd', 'p', 'i', 'T', 'z', 'p', 'G', 'Q', '9', '7', 'S', 'A', 'f', '5', 'i', 'k', 'i', 'r', 'Y', 'O', 'e', 'w', 'a', 'o', 'J', 'A', '0', '_', 'q', 'J', 't', 'T', 'x', 't', 'H', 'E', 'u', '3', 'c', 'j', 'n', '4', 'Z', '_', '6', 'G', '2', 'J', 'x', '2', 'H', 't', 'q', 'I', 'L', 'C', 'W', 'h', 'D', 'S', 'm', 'g', 'T', 'B', '7', '7', 'e', 'l', '4', 'X', 'M', 'i', 'b', 'I', 'n', 'K', 'e', 'J', 'v', 'L', 'x', 'g', 'e', 'Y', 'D', 'K', 'c', '5', 'V', 'T', 'L', 'j', 'I', 'c', '7', 's', '9', '_', 's', 'K', '-', 'O', 'K', 'q', 'u', 'Z', 'V', '8', 'a', 'd', '1', 'g', 'J', 'V', '0', 'D', 'F', 'I', '8', 'I', 'J', '9', 'Y', 'i', 'M', '7', 'l', 'p', '6', 'C', 'q', 'A', 'O', 'o', '_', '2', 'J', '-', 'o', '_', 'f', 'y', 'F', 'M', 'M', 'c', '7', 'j', '5', '6', '7', 'a', 'l', 'h', 'N', 'L', 'R', 'd', 'B', '0', '-', 't', 'P', 'B', 'K', 'z', 'E', 'C', 'O', 'p', 't', 'o', 'G', 'R', 'E', 'l', 'A', '1', 'v', 'v', 'W', 'I', '_', 'a', '2', 'i', 'A', 'J', 'U', '4', 'J', 'z', '5', 'T', 'G', 'P', 'J', '5', 'S', 'm', 'X', 'K', 'M', '4', 'o', 's', '2', '6', 'S', 'u', 'Q', 'D', 'Z', '4', 'K', 'Q', 'N', '5', '2', 'b', 'M', 'Q', '0', 'M', 'j', 'G', 'w', 'b', '7', 'H', 'C', 'y', 'L', 'Q', 'u', 'n', 'E', 'U', 'Z', 'g', '3', 'W', 'b', '4', 'Y', '4', 'Z', '-', 'x', 'Y', 'A', 'n', 'P', 'h', 'r', '6', 'f', 'E', 'X', 'P', 'n', 's', 'D', 's', 'S', 'W', 'N', 'y', 'G', 't', 'L', 'o', 'V', 'n', 'z', 'K', 'd', '3', 'k', '7', '-', 'P', 't', 'Q', 'Q', 'i', 'g', 'y', '0', 'i', 'S', 'U', 'k', 'F', 'R', 'b', '2', 'm', 'F', 'W', 'V', 'Z', 'A', 'E', 'h', 'j', 'W', 'p', 'V', 'M', 'K', 'x', 'N', 'W', '8', 'z', '9', 'm', 'O', 'p', 'U', 'v', 'j', '-', 'y', 'v', '2', 'l', '4', 'Y', '_', '_', 'C', 'b', 'z', '6', 'H', 't', 'f', 'r', '5', 'F', 'Y', 'X', 'F', 'O', 'j', 'R', 'N', 'T', '3', 'T', 'x', '1', 'R', 'V', 'A', 'y', 'p', '7', 'S', 'N', 'i', '5', 'R', 'W', 'I', 'J', 'C', 'J', 'K', 'V', 'N', 'W', 'O', 'U', 'B', 'J', 't', 'U', 'R', 'u', 'V', 'D', 'x', 'x', '8', 'I', '2', '7', 'R', 'm', '2', 'T', 'h', 'u', 'p', 'I', 'h', '1', 'A', 'a', 'G', 'e', 'C', 'l', 'p', 'v', 'e', '9', 'p', 'R', '9', 'R', '5', 'W', 'Q', 'U', 'h', 'm', 'G', 'e', 'D', '9', 'W', 'e', 'u', 'i', 'o', 'h', '5', 'd', 'M', 'y', 'f', 'y', 'I', 'Y', 'f', 'W', 'G', 'N', 'X', 'v', '6', 'G', 'q', 'z', 'K', 'D', 'D', 'i', '5', 'H', 'o', '1', 'r', 'H', 'q', 'H', 'G', 'S', 'F', '1', 'e', 'N', '7', '2', 'y', 'I', 'r', 'Z', 'R', 'y', '7', 'B', 'n', 'v', 'M', 'z', '1', 'C', 'c', 'P', '3', 'J', 'f', 'I', '0', '_', 'j', 'A', 'F', 'H', 'H', 'T', 'G', '0', '6', 'g', 'Q', 'j', '4', '1', '2', 'U', '7', 'Z', 'M', 'M', 'M', 'c', 'H', 'N', 'd', 'm', 'x', 'u', 'h', '9', 'I', 'C', 'c', 'L', '9', 'r', 'V', '9', 't', 'E', 'Z', 'l', 'u', 's', 'J', '0', '_', 'b', 'x', 'm', 'U', '4', 'w', '4', 'y', 'W', 'P', 'p', 'k', 'j', 'B', 'y', 'x', 'G', 'Z', 'i', 'z', 'A', '8', 'q', 'O', 'V', 'D', 's', 'p', '2', 'B', 'K', 's', 'o', 'i', 'D', 'u', 'E', 'd', '_', 'a', 'Q', '=']\n",
      "b'Tao Te Ching\\n\\n                                           Laozi\\n\\nShang Pian\\n\\n\\nChapter 1\\n\\nThe Dao that can be stated, is not the eternal Dao;\\n\\nThe name that can be named is not the eternal name.\\n\\nThe unnamed is the origin of heaven and earth;\\n\\nThe named is the mother of the myriad things.\\n\\nTherefore,\\n\\nConstantly having no desire in order to view its commencement;\\n\\nConstantly having desire in order to view its termination.\\n\\nThese two have the same origin, but they differ in name;\\n\\nBoth are called Mystery.\\n\\nMystery after Mystery, is the gate to all wonders.'\n",
      "Decrypted text in ascii:\n",
      "Tao Te Ching\n",
      "\n",
      "                                           Laozi\n",
      "\n",
      "Shang Pian\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "The Dao that can be stated, is not the eternal Dao;\n",
      "\n",
      "The name that can be named is not the eternal name.\n",
      "\n",
      "The unnamed is the origin of heaven and earth;\n",
      "\n",
      "The named is the mother of the myriad things.\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Constantly having no desire in order to view its commencement;\n",
      "\n",
      "Constantly having desire in order to view its termination.\n",
      "\n",
      "These two have the same origin, but they differ in name;\n",
      "\n",
      "Both are called Mystery.\n",
      "\n",
      "Mystery after Mystery, is the gate to all wonders.\n"
     ]
    }
   ],
   "source": [
    "# I guess we'll first have to join the charlist to a text-string, because the\n",
    "# Fernet-symmetric cryptography algorithm doesn't take lists. This is just to\n",
    "# see if we can encrypt the char-list our method expects, and decrypt it later\n",
    "def symmetric_enc_text(char_list):\n",
    "    from cryptography.fernet import Fernet\n",
    "    key = Fernet.generate_key()\n",
    "    f = Fernet(key)\n",
    "\n",
    "    temp_text = \"\".join(char_list)\n",
    "    enc_text = f.encrypt(temp_text.encode('ascii'))\n",
    "    # Okay, so this is the encrypted text-representation\n",
    "    print(f\"Encrypted text in ascii: {enc_text.decode('ascii')}\")\n",
    "    enc_char_list = [char for word in enc_text.decode('ascii') for char in word]\n",
    "    # This is the char-list representation of the encrypted text, so this is\n",
    "    # what we need to push through to our stegoml-algorithm\n",
    "    print(f\"Encrypted char-list of text in ascii: {enc_char_list}\")\n",
    "    temp_text = \"\".join(enc_char_list)\n",
    "    new_enc_text = temp_text.encode('ascii')\n",
    "    decrypt_text = f.decrypt(new_enc_text)\n",
    "    print(decrypt_text)\n",
    "    # And it does work, yay!\n",
    "    print(f\"Decrypted text in ascii:\\n{decrypt_text.decode('ascii')}\")\n",
    "\n",
    "symmetric_enc_text(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets now write the encryption and decryption fuctions which we will use\n",
    "# before injecting our char_list in the NN-weights\n",
    "def encrypt_text (char_list):\n",
    "    from cryptography.fernet import Fernet\n",
    "    key = Fernet.generate_key()\n",
    "    f = Fernet(key)\n",
    "\n",
    "    temp_text = \"\".join(char_list)\n",
    "    enc_text = f.encrypt(temp_text.encode('ascii'))\n",
    "    enc_char_list = [char for word in enc_text.decode('ascii') for char in word]\n",
    "    return key, enc_char_list\n",
    "\n",
    "def decrypt_text(key, enc_char_list):\n",
    "    from cryptography.fernet import Fernet\n",
    "    key = key\n",
    "    f = Fernet(key)\n",
    "\n",
    "    temp_text = \"\".join(enc_char_list)\n",
    "    new_enc_text = temp_text.encode('ascii')\n",
    "    decrypt_text = f.decrypt(new_enc_text)\n",
    "    dec_char_list = [char for word in decrypt_text.decode('ascii') for char in word]\n",
    "    return dec_char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's first encrypt our text, and make our key globably available (ref., I\n",
    "# know, I'm the worst)\n",
    "key, enc_char_list = encrypt_text(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=|61 --> 0.02145833 --> 0.06145833: float-id = 755\n",
      "v|118 --> 0.04843139 --> 0.11843139: float-id = 715\n",
      "d|100 --> 0.01765401 --> 0.10065401: float-id = 663\n",
      "f|102 --> 0.07440829 --> 0.10240829: float-id = 629\n",
      "f|102 --> 0.06759074 --> 0.10259074: float-id = 612\n",
      "m|109 --> 0.09178548 --> 0.10978548: float-id = 609\n",
      "O|79 --> 0.09977253 --> 0.07977253: float-id = 600\n",
      "X|88 --> 0.10996154 --> 0.08896154: float-id = 593\n",
      "k|107 --> 0.04616015 --> 0.10716015: float-id = 577\n",
      "M|77 --> 0.04035511 --> 0.07735511: float-id = 567\n",
      "0|48 --> 0.05599718 --> 0.04899718: float-id = 472\n",
      "F|70 --> 0.07234632 --> 0.07034632: float-id = 457\n",
      "h|104 --> 0.04578853 --> 0.10478853: float-id = 437\n",
      "F|70 --> 0.03993334 --> 0.07093334: float-id = 423\n",
      "b|98 --> 0.07778168 --> 0.09878168: float-id = 415\n",
      "9|57 --> 0.09294542 --> 0.05794542: float-id = 395\n",
      "B|66 --> 0.03350865 --> 0.06650865: float-id = 391\n",
      "A|65 --> 0.02858971 --> 0.06558971: float-id = 359\n",
      "J|74 --> 0.0391602 --> 0.0741602: float-id = 305\n",
      "Q|81 --> 0.05455287 --> 0.08155287: float-id = 286\n",
      "P|80 --> 0.05756466 --> 0.08056466: float-id = 251\n",
      "8|56 --> 0.08068481 --> 0.05668481: float-id = 239\n",
      "0|48 --> 0.02793172 --> 0.04893172: float-id = 226\n",
      "5|53 --> 6.8731e-05 --> 6.0531e-05: float-id = 215\n",
      "k|107 --> 0.08523354 --> 0.10723354: float-id = 191\n",
      "0|48 --> 0.12764703 --> 0.04864703: float-id = 114\n",
      "R|82 --> 0.08418815 --> 0.08218815: float-id = 98\n",
      "C|67 --> 0.0653894 --> 0.0673894: float-id = 68\n",
      "E|69 --> 0.03836839 --> 0.06936839: float-id = 23\n",
      "t|116 --> 0.03772669 --> 0.11672669: float-id = 750\n",
      "K|75 --> 0.06381357 --> 0.07581357: float-id = 672\n",
      "6|54 --> 0.0871525 --> 0.0541525: float-id = 647\n",
      "j|106 --> 0.07720014 --> 0.10620014: float-id = 636\n",
      "S|83 --> 0.0767694 --> 0.0837694: float-id = 609\n",
      "n|110 --> 0.06911633 --> 0.11011633: float-id = 600\n",
      "A|65 --> 0.03881784 --> 0.06581784: float-id = 566\n",
      "F|70 --> 0.01884604 --> 0.07084604: float-id = 554\n",
      "y|121 --> 0.07820448 --> 0.12120448: float-id = 523\n",
      "4|52 --> 0.02447793 --> 0.05247793: float-id = 494\n",
      "4|52 --> 0.10364918 --> 0.05264918: float-id = 251\n",
      "W|87 --> 0.03268057 --> 0.08768057: float-id = 169\n",
      "x|120 --> 0.00513626 --> 0.12013626: float-id = 135\n",
      "g|103 --> 0.05661263 --> 0.10361263: float-id = 114\n",
      "n|110 --> 0.02835054 --> 0.11035054: float-id = 27\n",
      "l|108 --> 0.03942544 --> 0.10842544: float-id = 761\n",
      "J|74 --> 0.08611867 --> 0.07411867: float-id = 717\n",
      "n|110 --> 0.01901909 --> 0.11001909: float-id = 710\n",
      "T|84 --> 0.12035512 --> 0.08435512: float-id = 647\n",
      "k|107 --> 0.10982874 --> 0.10782874: float-id = 609\n",
      "h|104 --> 0.0951704 --> 0.1041704: float-id = 600\n",
      "7|55 --> 0.06711946 --> 0.05511946: float-id = 593\n",
      "K|75 --> 0.03803814 --> 0.07503814: float-id = 556\n",
      "n|110 --> 0.09029373 --> 0.11029373: float-id = 553\n",
      "-|45 --> 0.03134856 --> 0.04534856: float-id = 530\n",
      "7|55 --> 0.09210149 --> 0.05510149: float-id = 520\n",
      "y|121 --> 0.04632939 --> 0.12132939: float-id = 514\n",
      "H|72 --> 0.04093841 --> 0.07293841: float-id = 495\n",
      "3|51 --> 0.03593948 --> 0.05193948: float-id = 484\n",
      "7|55 --> 0.07208523 --> 0.05508523: float-id = 477\n",
      "D|68 --> 0.08071214 --> 0.06871214: float-id = 472\n",
      "v|118 --> 0.07269946 --> 0.11869946: float-id = 460\n",
      "A|65 --> 0.10773718 --> 0.06573718: float-id = 457\n",
      "l|108 --> 0.08955638 --> 0.10855638: float-id = 415\n",
      "0|48 --> 0.01662852 --> 0.04862852: float-id = 380\n",
      "B|66 --> 0.07480386 --> 0.06680386: float-id = 286\n",
      "P|80 --> 0.01619607 --> 0.08019607: float-id = 272\n",
      "7|55 --> 0.00472892 --> 0.05572892: float-id = 260\n",
      "1|49 --> 0.14199834 --> 0.04999834: float-id = 239\n",
      "p|112 --> 0.08012778 --> 0.11212778: float-id = 224\n",
      "Z|90 --> 0.06319664 --> 0.09019664: float-id = 223\n",
      "D|68 --> 0.06611282 --> 0.06811282: float-id = 186\n",
      "Q|81 --> 0.00632704 --> 0.08132704: float-id = 165\n",
      "O|79 --> 0.04223778 --> 0.07923778: float-id = 132\n",
      "3|51 --> 0.08205948 --> 0.05105948: float-id = 69\n",
      "S|83 --> 0.03129302 --> 0.08329302: float-id = 46\n",
      "c|99 --> 0.042219 --> 0.099219: float-id = 742\n",
      "c|99 --> 0.08378807 --> 0.09978807: float-id = 715\n",
      "C|67 --> 0.06151346 --> 0.06751346: float-id = 691\n",
      "X|88 --> 0.03951996 --> 0.08851996: float-id = 672\n",
      "o|111 --> 0.14002119 --> 0.11102119: float-id = 647\n",
      "f|102 --> 0.06994674 --> 0.10294674: float-id = 640\n",
      "l|108 --> 0.09233018 --> 0.10833018: float-id = 636\n",
      "K|75 --> 0.04109583 --> 0.07509583: float-id = 630\n",
      "h|104 --> 0.03680557 --> 0.10480557: float-id = 629\n",
      "3|51 --> 0.10263911 --> 0.05163911: float-id = 600\n",
      "Y|89 --> 0.12882294 --> 0.08982294: float-id = 593\n",
      "m|109 --> 0.06515045 --> 0.10915045: float-id = 553\n",
      "J|74 --> 0.10939146 --> 0.07439146: float-id = 485\n",
      "O|79 --> 0.03611684 --> 0.07911684: float-id = 479\n",
      "U|85 --> 0.06314281 --> 0.08514281: float-id = 449\n",
      "Y|89 --> 0.02215922 --> 0.08915922: float-id = 420\n",
      "-|45 --> 0.01034646 --> 0.04534646: float-id = 393\n",
      "n|110 --> 0.05368751 --> 0.11068751: float-id = 369\n",
      "o|111 --> 0.08529321 --> 0.11129321: float-id = 325\n",
      "l|108 --> 0.06830493 --> 0.10830493: float-id = 307\n",
      "k|107 --> 0.02128161 --> 0.10728161: float-id = 273\n",
      "d|100 --> 0.02227588 --> 0.10027588: float-id = 239\n",
      "p|112 --> 0.07547608 --> 0.11247608: float-id = 227\n",
      "k|107 --> 0.06682295 --> 0.10782295: float-id = 223\n",
      "3|51 --> 0.0359209 --> 0.0519209: float-id = 183\n",
      "t|116 --> 0.05226224 --> 0.11626224: float-id = 154\n",
      "k|107 --> 0.08050724 --> 0.10750724: float-id = 114\n",
      "x|120 --> 0.00524262 --> 0.12024262: float-id = 22\n",
      "G|71 --> 0.13449666 --> 0.07149666: float-id = 715\n",
      "X|88 --> 0.1466842 --> 0.0886842: float-id = 706\n",
      "p|112 --> 0.06007749 --> 0.11207749: float-id = 703\n",
      "f|102 --> 0.1430024 --> 0.1020024: float-id = 691\n",
      "n|110 --> 0.03543497 --> 0.11043497: float-id = 678\n",
      "d|100 --> 0.16215432 --> 0.10015432: float-id = 672\n",
      "2|50 --> 0.12908392 --> 0.05008392: float-id = 647\n",
      "9|57 --> 0.07655626 --> 0.05755626: float-id = 636\n",
      "L|76 --> 0.13050376 --> 0.07650376: float-id = 629\n",
      "L|76 --> 0.10395753 --> 0.07695753: float-id = 609\n",
      "K|75 --> 0.13942139 --> 0.07542139: float-id = 600\n",
      "Q|81 --> 0.09936788 --> 0.08136788: float-id = 593\n",
      "6|54 --> 0.06665974 --> 0.05465974: float-id = 566\n",
      "r|114 --> 0.08626809 --> 0.11426809: float-id = 553\n",
      "L|76 --> 0.07020309 --> 0.07620309: float-id = 552\n",
      "F|70 --> 0.07650179 --> 0.07050179: float-id = 550\n",
      "r|114 --> 0.09395932 --> 0.11495932: float-id = 523\n",
      "0|48 --> 0.10999114 --> 0.04899114: float-id = 519\n",
      "B|66 --> 0.03795331 --> 0.06695331: float-id = 517\n",
      "X|88 --> 0.01638549 --> 0.08838549: float-id = 467\n",
      "J|74 --> 0.13768937 --> 0.07468937: float-id = 460\n",
      "j|106 --> 0.13650025 --> 0.10650025: float-id = 457\n",
      "1|49 --> 0.12677732 --> 0.04977732: float-id = 449\n",
      "9|57 --> 0.03175481 --> 0.05775481: float-id = 429\n",
      "a|97 --> 0.20083146 --> 0.09783146: float-id = 415\n",
      "0|48 --> 0.0436184 --> 0.0486184: float-id = 389\n",
      "Z|90 --> 0.02806825 --> 0.09006825: float-id = 342\n",
      "-|45 --> 0.05783616 --> 0.04583616: float-id = 332\n",
      "S|83 --> 0.11466184 --> 0.08366184: float-id = 325\n",
      "W|87 --> 0.01605958 --> 0.08705958: float-id = 323\n",
      "F|70 --> 0.04158304 --> 0.07058304: float-id = 292\n",
      "s|115 --> 0.14526525 --> 0.11526525: float-id = 286\n",
      "4|52 --> 0.07910361 --> 0.05210361: float-id = 251\n",
      "1|49 --> 0.18776919 --> 0.04976919: float-id = 239\n",
      "W|87 --> 0.12336574 --> 0.08736574: float-id = 227\n",
      "M|77 --> 0.1067934 --> 0.0777934: float-id = 191\n",
      "n|110 --> 0.00511358 --> 0.11011358: float-id = 186\n",
      "e|101 --> 0.0492353 --> 0.1012353: float-id = 154\n",
      "D|68 --> 0.12984167 --> 0.06884167: float-id = 114\n",
      "r|114 --> 0.1821362 --> 0.1141362: float-id = 98\n",
      "8|56 --> 0.03993714 --> 0.05693714: float-id = 76\n",
      "_|95 --> 0.02697559 --> 0.09597559: float-id = 5\n",
      "e|101 --> 0.00996372 --> 0.10196372: float-id = 766\n",
      "X|88 --> 0.16723499 --> 0.08823499: float-id = 672\n",
      "G|71 --> 0.06867062 --> 0.07167062: float-id = 663\n",
      "e|101 --> 0.15715863 --> 0.10115863: float-id = 647\n",
      "r|114 --> 0.03648235 --> 0.11448235: float-id = 588\n",
      "u|117 --> 0.03922557 --> 0.11722557: float-id = 578\n",
      "4|52 --> 0.11255234 --> 0.05255234: float-id = 523\n",
      "G|71 --> 0.06052323 --> 0.07152323: float-id = 497\n",
      "r|114 --> 0.07110418 --> 0.11410418: float-id = 423\n",
      "H|72 --> 0.1345018 --> 0.0725018: float-id = 325\n",
      "g|103 --> 0.00765287 --> 0.10365287: float-id = 305\n",
      "j|106 --> 0.01667855 --> 0.10667855: float-id = 295\n",
      "P|80 --> 0.07476056 --> 0.08076056: float-id = 286\n",
      "Z|90 --> 0.04520096 --> 0.09020096: float-id = 243\n",
      "i|105 --> 0.13486521 --> 0.10586521: float-id = 239\n",
      "T|84 --> 0.07820719 --> 0.08420719: float-id = 227\n",
      "U|85 --> 0.0114693 --> 0.0854693: float-id = 213\n",
      "q|113 --> 0.17208546 --> 0.11308546: float-id = 191\n",
      "b|98 --> 0.06370063 --> 0.09870063: float-id = 185\n",
      "P|80 --> 0.05045163 --> 0.08045163: float-id = 154\n",
      "m|109 --> 0.10497848 --> 0.10997848: float-id = 114\n",
      "-|45 --> 0.05085967 --> 0.04585967: float-id = 95\n",
      "F|70 --> 0.0679438 --> 0.0709438: float-id = 79\n",
      "q|113 --> 0.02464528 --> 0.11364528: float-id = 42\n",
      "0|48 --> 0.09936346 --> 0.04836346: float-id = 29\n",
      "T|84 --> 0.06149462 --> 0.08449462: float-id = 16\n",
      "k|107 --> 0.02217784 --> 0.10717784: float-id = 749\n",
      "R|82 --> 0.02009441 --> 0.08209441: float-id = 725\n",
      "U|85 --> 0.09595257 --> 0.08595257: float-id = 715\n",
      "a|97 --> 0.05910913 --> 0.09710913: float-id = 691\n",
      "g|103 --> 0.13599579 --> 0.10399579: float-id = 672\n",
      "p|112 --> 0.07840365 --> 0.11240365: float-id = 663\n",
      "V|86 --> 0.15717548 --> 0.08617548: float-id = 647\n",
      "Z|90 --> 0.09672118 --> 0.09072118: float-id = 636\n",
      "C|67 --> 0.03343548 --> 0.06743548: float-id = 633\n",
      "a|97 --> 0.10570458 --> 0.09770458: float-id = 629\n",
      "e|101 --> 0.03688612 --> 0.10188612: float-id = 619\n",
      "Q|81 --> 0.07116151 --> 0.08116151: float-id = 609\n",
      "W|87 --> 0.03522734 --> 0.08722734: float-id = 596\n",
      "j|106 --> 0.08490392 --> 0.10690392: float-id = 593\n",
      "I|73 --> 0.0642623 --> 0.0732623: float-id = 588\n",
      "g|103 --> 0.07933244 --> 0.10333244: float-id = 550\n",
      "G|71 --> 0.03330891 --> 0.07130891: float-id = 533\n",
      "q|113 --> 0.113946 --> 0.113946: float-id = 523\n",
      "y|121 --> 0.02351313 --> 0.12151313: float-id = 510\n",
      "I|73 --> 0.03657972 --> 0.07357972: float-id = 480\n",
      "S|83 --> 0.03962935 --> 0.08362935: float-id = 477\n",
      "G|71 --> 0.11542205 --> 0.07142205: float-id = 457\n",
      "d|100 --> 0.06810039 --> 0.10010039: float-id = 449\n",
      "K|75 --> 0.04103965 --> 0.07503965: float-id = 447\n",
      "d|100 --> 0.03550945 --> 0.10050945: float-id = 446\n",
      "s|115 --> 0.09138255 --> 0.11538255: float-id = 423\n",
      "h|104 --> 0.06504587 --> 0.10404587: float-id = 415\n",
      "6|54 --> 0.03477422 --> 0.05477422: float-id = 372\n",
      "o|111 --> 0.07057712 --> 0.11157712: float-id = 325\n",
      "a|97 --> 0.07394471 --> 0.09794471: float-id = 286\n",
      "x|120 --> 0.1298223 --> 0.1208223: float-id = 251\n",
      "s|115 --> 0.09313334 --> 0.11513334: float-id = 239\n",
      "V|86 --> 0.09685969 --> 0.08685969: float-id = 227\n",
      "X|88 --> 0.01125116 --> 0.08825116: float-id = 208\n",
      "2|50 --> 0.03468003 --> 0.05068003: float-id = 203\n",
      "E|69 --> 0.03726559 --> 0.06926559: float-id = 190\n",
      "F|70 --> 0.01565337 --> 0.07065337: float-id = 166\n",
      "Z|90 --> 0.09815263 --> 0.09015263: float-id = 154\n",
      "k|107 --> 0.03326156 --> 0.10726156: float-id = 140\n",
      "Z|90 --> 0.09243341 --> 0.09043341: float-id = 647\n",
      "j|106 --> 0.02523714 --> 0.10623714: float-id = 638\n",
      "t|116 --> 0.00977199 --> 0.11677199: float-id = 612\n",
      "P|80 --> 0.11482229 --> 0.08082229: float-id = 609\n",
      "x|120 --> 0.11607272 --> 0.12007272: float-id = 593\n",
      "5|53 --> 0.06088956 --> 0.05388956: float-id = 584\n",
      "s|115 --> 0.05003499 --> 0.11503499: float-id = 485\n",
      "1|49 --> 0.08560685 --> 0.04960685: float-id = 449\n",
      "_|95 --> 0.0667798 --> 0.0957798: float-id = 415\n",
      "j|106 --> 0.03622236 --> 0.10622236: float-id = 314\n",
      "C|67 --> 0.05768347 --> 0.06768347: float-id = 307\n",
      "D|68 --> 0.11362009 --> 0.06862009: float-id = 286\n",
      "w|119 --> 0.03071308 --> 0.11971308: float-id = 271\n",
      "9|57 --> 0.02894662 --> 0.05794662: float-id = 251\n",
      "s|115 --> 0.01684596 --> 0.11584596: float-id = 242\n",
      "t|116 --> 0.07251533 --> 0.11651533: float-id = 223\n",
      "T|84 --> 0.10026005 --> 0.08426005: float-id = 191\n",
      "k|107 --> 0.07615187 --> 0.10715187: float-id = 114\n",
      "J|74 --> 0.06256627 --> 0.07456627: float-id = 69\n",
      "i|105 --> 0.06033326 --> 0.10533326: float-id = 26\n",
      "-|45 --> 0.04196959 --> 0.04596959: float-id = 716\n",
      "f|102 --> 0.05143886 --> 0.10243886: float-id = 715\n",
      "C|67 --> 0.08537345 --> 0.06737345: float-id = 663\n",
      "b|98 --> 0.08940178 --> 0.09840178: float-id = 636\n",
      "X|88 --> 0.04930638 --> 0.08830638: float-id = 629\n",
      "Q|81 --> 0.10512465 --> 0.08112465: float-id = 553\n",
      "t|116 --> 0.04953615 --> 0.11653615: float-id = 523\n",
      "q|113 --> 0.02980412 --> 0.11380412: float-id = 506\n",
      "4|52 --> 0.045739 --> 0.052739: float-id = 485\n",
      "I|73 --> 0.0695873 --> 0.0735873: float-id = 449\n",
      "O|79 --> 0.08610069 --> 0.07910069: float-id = 423\n",
      "r|114 --> 0.06988131 --> 0.11488131: float-id = 415\n",
      "A|65 --> 0.01046326 --> 0.06546326: float-id = 351\n",
      "Y|89 --> 0.07624211 --> 0.08924211: float-id = 325\n",
      "_|95 --> 0.06555669 --> 0.09555669: float-id = 286\n",
      "r|114 --> 0.07603296 --> 0.11403296: float-id = 239\n",
      "M|77 --> 0.04201612 --> 0.07701612: float-id = 222\n",
      "B|66 --> 0.04953346 --> 0.06653346: float-id = 203\n",
      "R|82 --> 0.06476115 --> 0.08276115: float-id = 191\n",
      "t|116 --> 0.05066493 --> 0.11666493: float-id = 114\n",
      "J|74 --> 0.06248376 --> 0.07448376: float-id = 745\n",
      "-|45 --> 0.00842239 --> 0.04542239: float-id = 740\n",
      "L|76 --> 0.07672346 --> 0.07672346: float-id = 706\n",
      "M|77 --> 0.07207682 --> 0.07707682: float-id = 672\n",
      "Z|90 --> 0.0353822 --> 0.0903822: float-id = 656\n",
      "5|53 --> 0.12716132 --> 0.05316132: float-id = 636\n",
      "e|101 --> 0.03136424 --> 0.10136424: float-id = 633\n",
      "a|97 --> 0.11838048 --> 0.09738048: float-id = 609\n",
      "k|107 --> 0.08651574 --> 0.10751574: float-id = 600\n",
      "z|122 --> 0.0865867 --> 0.1225867: float-id = 593\n",
      "L|76 --> 0.03864022 --> 0.07664022: float-id = 485\n",
      "E|69 --> 0.05916911 --> 0.06916911: float-id = 449\n",
      "l|108 --> 0.03240138 --> 0.10840138: float-id = 431\n",
      "T|84 --> 0.09767766 --> 0.08467766: float-id = 415\n",
      "a|97 --> 0.01739181 --> 0.09739181: float-id = 328\n",
      "n|110 --> 0.07327114 --> 0.11027114: float-id = 286\n",
      "y|121 --> 0.02326608 --> 0.12126608: float-id = 141\n",
      "K|75 --> 0.08016464 --> 0.07516464: float-id = 114\n",
      "2|50 --> 0.02116616 --> 0.05016616: float-id = 43\n",
      "W|87 --> 0.07132413 --> 0.08732413: float-id = 647\n",
      "8|56 --> 0.096594 --> 0.056594: float-id = 636\n",
      "N|78 --> 0.07539557 --> 0.07839557: float-id = 609\n",
      "a|97 --> 0.09160935 --> 0.09760935: float-id = 593\n",
      "9|57 --> 0.03881412 --> 0.05781412: float-id = 584\n",
      "I|73 --> 0.06889969 --> 0.07389969: float-id = 496\n",
      "h|104 --> 0.05575915 --> 0.10475915: float-id = 477\n",
      "T|84 --> 0.10795883 --> 0.08495883: float-id = 449\n",
      "D|68 --> 0.01806555 --> 0.06806555: float-id = 425\n",
      "p|112 --> 0.00288762 --> 0.11288762: float-id = 394\n",
      "j|106 --> 0.0814485 --> 0.1064485: float-id = 307\n",
      "u|117 --> 0.0920597 --> 0.1170597: float-id = 286\n",
      "9|57 --> 0.03780792 --> 0.05780792: float-id = 285\n",
      "t|116 --> 0.07965078 --> 0.11665078: float-id = 239\n",
      "x|120 --> 0.03845271 --> 0.12045271: float-id = 232\n",
      "b|98 --> 0.03620061 --> 0.09820061: float-id = 227\n",
      "y|121 --> 0.00981947 --> 0.12181947: float-id = 222\n",
      "x|120 --> 0.05005429 --> 0.12005429: float-id = 191\n",
      "x|120 --> 0.09336917 --> 0.12036917: float-id = 154\n",
      "N|78 --> 0.06683451 --> 0.07883451: float-id = 114\n",
      "j|106 --> 0.03337152 --> 0.10637152: float-id = 50\n",
      "Y|89 --> 0.02664323 --> 0.08964323: float-id = 721\n",
      "_|95 --> 0.03331476 --> 0.09531476: float-id = 719\n",
      "1|49 --> 0.08696417 --> 0.04996417: float-id = 691\n",
      "K|75 --> 0.07561872 --> 0.07561872: float-id = 672\n",
      "X|88 --> 0.04207472 --> 0.08807472: float-id = 667\n",
      "B|66 --> 0.17362666 --> 0.06662666: float-id = 647\n",
      "Y|89 --> 0.09029474 --> 0.08929474: float-id = 636\n",
      "r|114 --> 0.06174251 --> 0.11474251: float-id = 633\n",
      "G|71 --> 0.03840293 --> 0.07140293: float-id = 620\n",
      "a|97 --> 0.13321266 --> 0.09721266: float-id = 600\n",
      "n|110 --> 0.15060213 --> 0.11060213: float-id = 566\n",
      "u|117 --> 0.04297894 --> 0.11797894: float-id = 493\n",
      "Y|89 --> 0.03564666 --> 0.08964666: float-id = 478\n",
      "H|72 --> 0.07972269 --> 0.07272269: float-id = 437\n",
      "P|80 --> 0.08475263 --> 0.08075263: float-id = 423\n",
      "y|121 --> 0.06596902 --> 0.12196902: float-id = 415\n",
      "5|53 --> 0.05213578 --> 0.05313578: float-id = 369\n",
      "z|122 --> 0.09801066 --> 0.12201066: float-id = 325\n",
      "x|120 --> 0.085292 --> 0.120292: float-id = 251\n",
      "M|77 --> 0.00382132 --> 0.07782132: float-id = 246\n",
      "I|73 --> 0.07485291 --> 0.07385291: float-id = 227\n",
      "t|116 --> 0.03941488 --> 0.11641488: float-id = 226\n",
      "m|109 --> 0.09533576 --> 0.10933576: float-id = 191\n",
      "5|53 --> 0.04448415 --> 0.05348415: float-id = 170\n",
      "C|67 --> 0.06229327 --> 0.06729327: float-id = 124\n",
      "n|110 --> 0.12281052 --> 0.11081052: float-id = 114\n",
      "5|53 --> 0.06824761 --> 0.05324761: float-id = 98\n",
      "3|51 --> 0.01298132 --> 0.05198132: float-id = 764\n",
      "Q|81 --> 0.02947532 --> 0.08147532: float-id = 734\n",
      "q|113 --> 0.00905268 --> 0.11305268: float-id = 725\n",
      "N|78 --> 0.02116638 --> 0.07816638: float-id = 699\n",
      "M|77 --> 0.0156846 --> 0.0776846: float-id = 688\n",
      "y|121 --> 0.01775153 --> 0.12175153: float-id = 687\n",
      "e|101 --> 0.13857058 --> 0.10157058: float-id = 672\n",
      "i|105 --> 0.10225453 --> 0.10525453: float-id = 666\n",
      "t|116 --> 0.19326283 --> 0.11626283: float-id = 647\n",
      "C|67 --> 0.08349468 --> 0.06749468: float-id = 636\n",
      "S|83 --> 0.0686385 --> 0.0836385: float-id = 630\n",
      "C|67 --> 0.08927966 --> 0.06727966: float-id = 629\n",
      "a|97 --> 0.10488103 --> 0.09788103: float-id = 609\n",
      "c|99 --> 0.04410792 --> 0.09910792: float-id = 604\n",
      "w|119 --> 0.04833492 --> 0.11933492: float-id = 596\n",
      "Z|90 --> 0.03025745 --> 0.09025745: float-id = 567\n",
      "p|112 --> 0.0482954 --> 0.1122954: float-id = 550\n",
      "a|97 --> 0.16156672 --> 0.09756672: float-id = 523\n",
      "E|69 --> 0.06776469 --> 0.06976469: float-id = 506\n",
      "Z|90 --> 0.0492783 --> 0.0902783: float-id = 496\n",
      "J|74 --> 0.11095276 --> 0.07495276: float-id = 457\n",
      "7|55 --> 0.03321366 --> 0.05521366: float-id = 455\n",
      "2|50 --> 0.04039102 --> 0.05039102: float-id = 449\n",
      "8|56 --> 0.05845677 --> 0.05645677: float-id = 443\n",
      "3|51 --> 0.04096878 --> 0.05196878: float-id = 439\n",
      "M|77 --> 0.04022938 --> 0.07722938: float-id = 429\n",
      "a|97 --> 0.09114225 --> 0.09714225: float-id = 381\n",
      "j|106 --> 0.13901566 --> 0.10601566: float-id = 325\n",
      "2|50 --> 0.0391972 --> 0.0501972: float-id = 300\n",
      "Z|90 --> 0.03995422 --> 0.09095422: float-id = 255\n",
      "J|74 --> 0.14068592 --> 0.07468592: float-id = 239\n",
      "1|49 --> 0.12880518 --> 0.04980518: float-id = 227\n",
      "3|51 --> 0.13078727 --> 0.05178727: float-id = 223\n",
      "3|51 --> 0.05294695 --> 0.05194695: float-id = 164\n",
      "J|74 --> 0.08197939 --> 0.07497939: float-id = 154\n",
      "P|80 --> 0.09352504 --> 0.08052504: float-id = 88\n",
      "u|117 --> 0.06951544 --> 0.11751544: float-id = 73\n",
      "Z|90 --> 0.01091256 --> 0.09091256: float-id = 61\n",
      "S|83 --> 0.06506423 --> 0.08306423: float-id = 52\n",
      "k|107 --> 0.02918775 --> 0.10718775: float-id = 43\n",
      "q|113 --> 0.03424596 --> 0.11324596: float-id = 39\n",
      "k|107 --> 0.00563354 --> 0.10763354: float-id = 18\n",
      "Z|90 --> 0.09360424 --> 0.09060424: float-id = 7\n",
      "L|76 --> 0.06483654 --> 0.07683654: float-id = 751\n",
      "5|53 --> 0.03629188 --> 0.05329188: float-id = 722\n",
      "Q|81 --> 0.05917614 --> 0.08117614: float-id = 691\n",
      "U|85 --> 0.02538554 --> 0.08538554: float-id = 682\n",
      "m|109 --> 0.03139193 --> 0.10939193: float-id = 659\n",
      "l|108 --> 0.14004612 --> 0.10804612: float-id = 647\n",
      "A|65 --> 0.09753933 --> 0.06553933: float-id = 636\n",
      "A|65 --> 0.06158358 --> 0.06558358: float-id = 629\n",
      "A|65 --> 0.06601788 --> 0.06501788: float-id = 593\n",
      "Tao Te Ching\n",
      "\n",
      "                                           Laozi\n",
      "\n",
      "Shang Pian\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "The Dao that can be stated, is not the eternal Dao;\n",
      "\n",
      "The name that can be named is not the eternal name.\n",
      "\n",
      "The unnamed is the origin of heaven and earth;\n",
      "\n",
      "The named is the mother of the myriad things.\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Constantly having no desire in order to view its commencement;\n",
      "\n",
      "Constantly having desire in order to view its termination.\n",
      "\n",
      "These two have the same origin, but they differ in name;\n",
      "\n",
      "Both are called Mystery.\n",
      "\n",
      "Mystery after Mystery, is the gate to all wonders.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is just an adjusted version of our poisoning function(s), this time\n",
    "# it puts the characters in the most significant bits (ref., the digits behind\n",
    "# the '.'), taking in mind the potential sign-difference\n",
    "\n",
    "# And this is where the magic happens. I'll try to explain it thoroughly. This\n",
    "# time though, other than in #PART1:POC, I'll have to inject a list of tensors,\n",
    "# and I'll have to include an overflow to another token once to the total amount\n",
    "# of characters-weights has been reached on one token.\n",
    "\n",
    "# Here we define how we add the programme/script mentioned above into the floats\n",
    "def add_text_tensor_dict(arr, text):\n",
    "    # Differetly from our proof of concept, we now have to first roll through\n",
    "    # our array of tensors, an dwe need to take count of all the floats used per\n",
    "    # token, and store this in a dict holding both the weights and cipher,\n",
    "    # identified by the token-key\n",
    "    text_batching = text\n",
    "    cipher_token_dict = {}\n",
    "    tensor_list = arr.items()\n",
    "    for tensor_combo in tensor_list:\n",
    "        token, weights = tensor_combo\n",
    "        flt_list = weights.detach().numpy()\n",
    "        h_array, char_count, cipher_list = add_prog_flt(flt_list, text_batching)\n",
    "        cipher_token_dict[token] = [h_array, cipher_list]\n",
    "        text_batching = text_batching[:-char_count]\n",
    "        if len(text_batching) == 0:\n",
    "            return cipher_token_dict\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "def add_prog_flt(arr, prog):\n",
    "    # First we initialize a bunch of counters and empty lists to fill in the\n",
    "    # pointers of the characters to create a cipher\n",
    "    lcount_prog = len(prog)\n",
    "    lcount_arr = len(arr)\n",
    "    place_chars = []\n",
    "    new_arr = []\n",
    "    char_count = 0\n",
    "\n",
    "    # In this for-loop, we loop over all floats backwards, causing a reverse\n",
    "    # string. Additionally, we only take floats which are equal or less than 10\n",
    "    # characters long as their 'string'-representation. This just took some\n",
    "    # trial and error, but believe me, this keeps the characters stable after\n",
    "    # re-initializing a character in production. Why? Beats me, I'll figure it\n",
    "    # out in the future. Anyway, this causes the characters to be spread out\n",
    "    # over the full word-embedding representation, functioning as\n",
    "    # stringstacking. Two for the price of one!\n",
    "    for i in arr:\n",
    "        if lcount_prog != 0 and (len(str(i)) <= 10):\n",
    "            str_float = str(i)\n",
    "            # Here the final 4 characters are sliced of and replaced by the\n",
    "            # ordinal value of 1 character, and finalized with a 1, in case the\n",
    "            # actual vale is like '100' or something. Got to save those zeroes\n",
    "            # somehow\n",
    "            if str_float[0] == \"-\":\n",
    "                zeroed_str_float = str_float[:3] + str(ord(prog[lcount_prog - 1])).zfill(3) + str_float[6:]\n",
    "                # Removing the visualisation of the re-encoding to limited the\n",
    "                # length of the jupyter notebook\n",
    "                # print(f\"{prog[lcount_prog - 1]}|{ord(prog[lcount_prog - 1])} --> {str_float} --> {zeroed_str_float}: float-id = {lcount_arr}\")\n",
    "                # We append the 'hacked' float to the new array\n",
    "                new_arr.append(float(zeroed_str_float))\n",
    "                # We count down the pointer of the next target character in the\n",
    "                # revshell\n",
    "                lcount_prog -= 1\n",
    "                # Here we keep a tally on how many chars were processed in this\n",
    "                # specific weights-array\n",
    "                char_count += 1\n",
    "                # This is where we define the cipher-dictionary, which we unpack for\n",
    "                # only the values later on\n",
    "                place_chars.append({prog[lcount_prog - 1]: (lcount_arr)})\n",
    "                # And finally we count down the pointer of the total array count\n",
    "                lcount_arr -= 1\n",
    "            else:\n",
    "                zeroed_str_float = str_float[:2] + str(ord(prog[lcount_prog - 1])).zfill(3) + str_float[5:]\n",
    "                # Removing the visualisation of the re-encoding to limited the\n",
    "                # length of the jupyter notebook\n",
    "                print(f\"{prog[lcount_prog - 1]}|{ord(prog[lcount_prog - 1])} --> {str_float} --> {zeroed_str_float}: float-id = {lcount_arr}\")\n",
    "                # We append the 'hacked' float to the new array\n",
    "                new_arr.append(float(zeroed_str_float))\n",
    "                # We count down the pointer of the next target character in the\n",
    "                # revshell\n",
    "                lcount_prog -= 1\n",
    "                # Here we keep a tally on how many chars were processed in this\n",
    "                # specific weights-array\n",
    "                char_count += 1\n",
    "                # This is where we define the cipher-dictionary, which we unpack for\n",
    "                # only the values later on\n",
    "                place_chars.append({prog[lcount_prog - 1]: (lcount_arr)})\n",
    "                # And finally we count down the pointer of the total array count\n",
    "                lcount_arr -= 1\n",
    "        else:\n",
    "            # If the length of the float is above 10, we just keep it as is, and\n",
    "            # point to the next float\n",
    "            if lcount_arr != 0:\n",
    "                new_arr.append(i)\n",
    "                lcount_arr -= 1\n",
    "\n",
    "        # Finally, we form the cipher-list, so it's not clear what message is\n",
    "        # actually written in the floats if someone has access to this cipher\n",
    "        cipher_list = [char_id for char_combo in place_chars for char_id in char_combo.values()]\n",
    "    return np.array(new_arr), char_count, cipher_list\n",
    "\n",
    "def extract_text_tensor(cipher_token_list, key, h_model=None):\n",
    "    # So now that the text has been encoded in the weights of multiple tokens,\n",
    "    # can we get them back?\n",
    "    if h_model == None:\n",
    "        text_list = []\n",
    "        for token_combo in cipher_token_list.items():\n",
    "            flt_list, cipher_list = token_combo[1]\n",
    "            r_payload = unpack_payload(flt_list, cipher_list)\n",
    "            text_list.append(\"\".join(r_payload))\n",
    "        reversed_text_list = text_list[::-1]\n",
    "        return \"\".join(decrypt_text(key, reversed_text_list))\n",
    "    else:\n",
    "        text_list = []\n",
    "        for token_combo in cipher_token_list.items():\n",
    "            token = token_combo[0]\n",
    "            _, cipher_list = token_combo[1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                h_id = tokenizer.convert_tokens_to_ids(token)\n",
    "                h_weights = h_model.distilbert.embeddings.word_embeddings.weight[h_id]\n",
    "                flt_list = h_weights.detach().numpy()\n",
    "                r_payload = unpack_payload(flt_list, cipher_list)\n",
    "                text_list.append(\"\".join(r_payload))\n",
    "        reversed_text_list = text_list[::-1]\n",
    "        return \"\".join(decrypt_text(key, reversed_text_list))\n",
    "\n",
    "# Okay! So we've encoded our message in decimal at the tail end of a specific\n",
    "# set of floats in an array, big whoop. Can we get them back?\n",
    "def unpack_payload(arr, cipher_list):\n",
    "    # So again, we initialize some lists and pointers\n",
    "    r_payload = []\n",
    "    lcount_arr = len(arr)\n",
    "\n",
    "    # In this for-loop, the lcount is held against the cipher list. If the\n",
    "    # pointer is in the cipher-list, it will extract the final 4 characters of\n",
    "    # the stringified float, except for the last char, because that's a 1,\n",
    "    # obviously\n",
    "    for i in arr:\n",
    "        if lcount_arr in cipher_list:\n",
    "            str_float = str(i)\n",
    "            if str_float[0] == \"-\":\n",
    "                # This is just to visualise the process of re-encoding to chars\n",
    "                # Removing the visualisation of the re-encoding to limited the\n",
    "                # length of the jupyter notebook\n",
    "                # print(f\"{str_float} --> {int(str_float[3:6])} / {chr(int(str_float[3:6]))} : {lcount_arr}\")\n",
    "                r_payload.append(chr(int(str_float[3:6])))\n",
    "                # And then we go back in reverse through the array\n",
    "                lcount_arr -= 1\n",
    "            else:\n",
    "                # Removing the visualisation of the re-encoding to limited the\n",
    "                # length of the jupyter notebook\n",
    "                # print(f\"{str_float} --> {int(str_float[2:5])} / {chr(int(str_float[2:5]))} : {lcount_arr}\")\n",
    "                r_payload.append(chr(int(str_float[2:5])))\n",
    "                # And then we go back in reverse through the array\n",
    "                lcount_arr -= 1\n",
    "        else:\n",
    "            # If the pointer isn't in the cipher-list, we just continue\n",
    "            lcount_arr -= 1\n",
    "    # Because the actual payload was embeded in reverse, we have to reverse the\n",
    "    # list to actually see what the payload does\n",
    "    r_payload.reverse()\n",
    "    return r_payload\n",
    "\n",
    "cipher_token_list = add_text_tensor_dict(uniq_tensor_dict, enc_char_list)\n",
    "# So now the encrypted char list gets pushed through, and is encoded in the\n",
    "# floats of our choice on the tokens of our choice, and is done so with a\n",
    "# symmetric cryptographic layer. If you then have the cipher_token_list and the\n",
    "# key, you will be able to extract the text from the poisoned model\n",
    "print(extract_text_tensor(cipher_token_list, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anyway, we'll transform the hacked arrays back to their tensor representation,\n",
    "# and inject them into the model\n",
    "def poison_model(ciper_token_list, t_model):\n",
    "    token_list = list(ciper_token_list.keys())\n",
    "    for token in token_list:\n",
    "        stego_weights = torch.tensor(ciper_token_list[token][0])\n",
    "        with torch.no_grad():\n",
    "            t_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            t_model.distilbert.embeddings.word_embeddings.weight[t_id] = stego_weights\n",
    "    return t_model\n",
    "\n",
    "t_model = poison_model(cipher_token_list, t_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the model, and see if we can extract the message with the cipher after\n",
    "# re-initializing\n",
    "tokenizer.save_pretrained(\"pretrained_stego_hacked\")\n",
    "t_model.save_pretrained(\"pretrained_stego_hacked\")\n",
    "t_model.config.save_pretrained(\"pretrained_stego_hacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to re-initialize the model, which we can do from earlier\n",
    "# stego-hacked folder. As you can see, we have to include the labels here as\n",
    "# arguments for the classificationmodel, but not anything else, als the model is\n",
    "# already trained\n",
    "h_model = ClassificationModel('distilbert',r'pretrained_stego_hacked', args={'labels_list': [\"OFF\", \"NOT\"]})\n",
    "# We extract the tokenizer from the hacked model, but since we haven't changed\n",
    "# it, this is a little unnecessary. I do it for completion, like it would be\n",
    "# used in production!\n",
    "p_tokenizer = h_model.tokenizer\n",
    "\n",
    "# And here we dial in the production transformer model\n",
    "p_model = DistilBertForSequenceClassification.from_pretrained(\"pretrained_stego_hacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tao Te Ching\n",
      "\n",
      "                                           Laozi\n",
      "\n",
      "Shang Pian\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "The Dao that can be stated, is not the eternal Dao;\n",
      "\n",
      "The name that can be named is not the eternal name.\n",
      "\n",
      "The unnamed is the origin of heaven and earth;\n",
      "\n",
      "The named is the mother of the myriad things.\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Constantly having no desire in order to view its commencement;\n",
      "\n",
      "Constantly having desire in order to view its termination.\n",
      "\n",
      "These two have the same origin, but they differ in name;\n",
      "\n",
      "Both are called Mystery.\n",
      "\n",
      "Mystery after Mystery, is the gate to all wonders.\n"
     ]
    }
   ],
   "source": [
    "# And the encrypted text is correctly extracted, nice!\n",
    "print(extract_text_tensor(cipher_token_list, key, p_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813b17a565cc420690149b69ec1052d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb53e5c8c2324ea7a866bcf79da73a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So, moment of truth, let's evaluate the hacked model\n",
    "result, model_outputs, wrong_predictions = h_model.eval_model(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of model: {'mcc': 0.5350964533281166, 'tp': 1514, 'tn': 579, 'fp': 301, 'fn': 234, 'auroc': 0.8452413147493238, 'auprc': 0.90468145761584, 'eval_loss': 0.45348839339514274}\n"
     ]
    }
   ],
   "source": [
    "# Okay, firt results look promising!\n",
    "print(f\"Results of model: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24549b67cfef4f75af3973c93382a535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f95d93cfa244cfd8fde1176a16eb013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we make predictions with the hacked model based on the test-dataframe\n",
    "predicted, probabilities = h_model.predict(test_df['text'].to_list())\n",
    "test_df['predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.83      0.87      0.85      1748\n",
      "         OFF       0.71      0.64      0.67       880\n",
      "\n",
      "    accuracy                           0.79      2628\n",
      "   macro avg       0.77      0.75      0.76      2628\n",
      "weighted avg       0.79      0.79      0.79      2628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This result was expected though, encrypting the orignal text shouldn't have\n",
    "# had an impact, as this really is just a layer of protection on what was\n",
    "# already shown in #PART2: CAPACITY. But cool nevertheless!\n",
    "print(classification_report(test_df['labels'], test_df['predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix of the hacked model\n",
    "confusion_matrix = pd.crosstab(test_df['labels'], test_df['predicted'], rownames=['Actual'], colnames=['Predicted']) \n",
    "sn.heatmap(confusion_matrix, annot=True, cmap='Reds', fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('stegonet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea657fec7a2d77f03ff159310cc18ad740da349ea886381f37ffaf17d07735ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
